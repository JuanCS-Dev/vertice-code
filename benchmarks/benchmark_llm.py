"""Benchmark script for LLM performance metrics."""

import asyncio
import time
from qwen_dev_cli.core.llm import llm_client


async def benchmark_performance():
    """Benchmark TTFT and throughput."""
    print("âš¡ LLM Performance Benchmark\n")
    print("="*60)
    
    prompt = "Write a Python function to calculate fibonacci numbers"
    
    print(f"\nğŸ“¤ Prompt: {prompt}\n")
    print("â±ï¸  Measuring performance...\n")
    
    start_time = time.time()
    first_token_time = None
    chunks_received = 0
    total_chars = 0
    
    async for chunk in llm_client.stream_chat(prompt):
        chunks_received += 1
        total_chars += len(chunk)
        
        # Measure TTFT (Time to First Token)
        if first_token_time is None:
            first_token_time = time.time()
            ttft = (first_token_time - start_time) * 1000
            print(f"âš¡ TTFT: {ttft:.0f}ms")
            print("\nğŸ“¥ Response:\n")
        
        print(chunk, end="", flush=True)
    
    end_time = time.time()
    total_time = end_time - start_time
    generation_time = end_time - first_token_time if first_token_time else total_time
    
    # Calculate metrics
    tokens_approx = total_chars // 4  # Rough estimate
    throughput = tokens_approx / generation_time if generation_time > 0 else 0
    
    print("\n\n" + "="*60)
    print("\nğŸ“Š Performance Metrics:\n")
    print(f"âš¡ TTFT (Time to First Token): {ttft:.0f}ms")
    print(f"â±ï¸  Total Time: {total_time:.2f}s")
    print(f"ğŸ“ Total Characters: {total_chars}")
    print(f"ğŸ¯ Chunks Received: {chunks_received}")
    print(f"ğŸ“Š Approx Tokens: {tokens_approx}")
    print(f"ğŸš€ Throughput: {throughput:.1f} tokens/sec")
    
    # Validate targets
    print("\nğŸ¯ Target Validation:\n")
    print(f"TTFT Target: <2000ms â†’ {'âœ… PASS' if ttft < 2000 else 'âŒ FAIL'} ({ttft:.0f}ms)")
    print(f"Throughput Target: >10 t/s â†’ {'âœ… PASS' if throughput > 10 else 'âŒ FAIL'} ({throughput:.1f} t/s)")


async def main():
    """Run benchmark."""
    try:
        await benchmark_performance()
        print("\n\nğŸ‰ Benchmark complete!")
    except Exception as e:
        print(f"\nâŒ Benchmark failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
