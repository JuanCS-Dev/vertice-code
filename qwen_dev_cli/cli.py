"""Command-line interface for qwen-dev-cli."""

import asyncio
from pathlib import Path
from typing import Optional, List
import typer
from rich.console import Console
from rich.markdown import Markdown
from rich.progress import Progress, SpinnerColumn, TextColumn

from .core.llm import llm_client
from .core.context import context_builder
from .core.mcp import mcp_manager
from .core.config import config

app = typer.Typer(
    name="qwen-dev",
    help="AI-Powered Code Assistant with MCP Integration",
    add_completion=False
)
console = Console()


@app.command()
def explain(
    file_path: str = typer.Argument(..., help="Path to file to explain"),
    context_files: Optional[List[str]] = typer.Option(
        None, "--context", "-c", help="Additional context files"
    ),
    model: Optional[str] = typer.Option(None, "--model", "-m", help="LLM model to use"),
):
    """Explain code from a file."""
    console.print(f"\n[bold blue]üîç Explaining:[/bold blue] {file_path}\n")
    
    # Read main file
    success, content, error = context_builder.read_file(file_path)
    
    if not success:
        console.print(f"[bold red]‚ùå Error:[/bold red] {error}")
        raise typer.Exit(1)
    
    # Add context files if provided
    if context_files:
        console.print(f"[dim]üìÇ Loading context files...[/dim]")
        results = context_builder.add_files(context_files)
        for file, result in results.items():
            console.print(f"  {result}")
    
    # Build prompt
    prompt = f"Please explain this code:\n\n```\n{content}\n```"
    
    if context_files:
        context = context_builder.get_context()
        prompt = context_builder.inject_to_prompt(prompt)
    
    # Generate explanation
    with console.status("[bold green]Generating explanation...", spinner="dots"):
        response = asyncio.run(llm_client.generate(prompt))
    
    console.print("\n[bold green]üìù Explanation:[/bold green]\n")
    console.print(Markdown(response))
    
    # Clear context for next use
    context_builder.clear()


@app.command()
def generate(
    prompt: str = typer.Argument(..., help="What code to generate"),
    context_files: Optional[List[str]] = typer.Option(
        None, "--context", "-c", help="Context files for reference"
    ),
    output: Optional[str] = typer.Option(None, "--output", "-o", help="Save to file"),
    stream: bool = typer.Option(True, "--stream/--no-stream", help="Stream output"),
):
    """Generate code based on a prompt."""
    console.print(f"\n[bold blue]‚ú® Generating:[/bold blue] {prompt}\n")
    
    # Add context files if provided
    if context_files:
        console.print(f"[dim]üìÇ Loading context files...[/dim]")
        results = context_builder.add_files(context_files)
        for file, result in results.items():
            console.print(f"  {result}")
        
        context = context_builder.get_context()
        prompt = context_builder.inject_to_prompt(prompt)
    
    # Generate code
    console.print("[bold green]üìù Generated Code:[/bold green]\n")
    
    if stream:
        # Stream output
        async def stream_generation():
            async for chunk in llm_client.stream_chat(prompt):
                console.print(chunk, end="")
        
        asyncio.run(stream_generation())
    else:
        # Non-streaming
        with console.status("[bold green]Generating code...", spinner="dots"):
            response = asyncio.run(llm_client.generate(prompt))
        console.print(Markdown(response))
    
    console.print("\n")
    
    # Save to file if requested
    if output:
        Path(output).write_text(response)
        console.print(f"[bold green]‚úÖ Saved to:[/bold green] {output}")
    
    # Clear context
    context_builder.clear()


@app.command()
def serve(
    port: int = typer.Option(7860, "--port", "-p", help="Port for web UI"),
    share: bool = typer.Option(False, "--share", help="Create public share link"),
):
    """Start the Gradio web UI."""
    console.print(f"\n[bold blue]üöÄ Starting web UI on port {port}...[/bold blue]\n")
    
    # Import here to avoid loading Gradio unless needed
    try:
        from .ui import create_ui
        
        demo = create_ui()
        demo.launch(server_port=port, share=share)
        
    except ImportError as e:
        console.print(f"[bold red]‚ùå Error:[/bold red] Gradio not installed")
        console.print(f"[dim]Install with: pip install gradio>=6.0.0[/dim]")
        raise typer.Exit(1)
    except Exception as e:
        console.print(f"[bold red]‚ùå Error:[/bold red] {e}")
        raise typer.Exit(1)


@app.command()
def version():
    """Show version information."""
    console.print("\n[bold]qwen-dev-cli[/bold] v0.1.0")
    console.print("AI-Powered Code Assistant with MCP Integration")
    console.print("\n[dim]Powered by:[/dim]")
    console.print(f"  ‚Ä¢ Model: {config.hf_model}")
    console.print(f"  ‚Ä¢ MCP: {'Enabled' if mcp_manager.enabled else 'Disabled'}")
    console.print()


@app.command()
def config_show():
    """Show current configuration."""
    console.print("\n[bold]üìã Configuration:[/bold]\n")
    
    # LLM settings
    console.print("[bold blue]LLM Settings:[/bold blue]")
    console.print(f"  Model: {config.hf_model}")
    console.print(f"  Max Tokens: {config.max_tokens}")
    console.print(f"  Temperature: {config.temperature}")
    console.print(f"  Streaming: {config.stream}")
    
    # Context settings
    console.print("\n[bold blue]Context Settings:[/bold blue]")
    console.print(f"  Max Files: {config.max_context_files}")
    console.print(f"  Max File Size: {config.max_file_size_kb}KB")
    
    # MCP settings
    console.print("\n[bold blue]MCP Settings:[/bold blue]")
    stats = mcp_manager.get_stats()
    console.print(f"  Enabled: {stats['enabled']}")
    console.print(f"  Root Dir: {stats['root_dir']}")
    console.print(f"  Files in Context: {stats['files']}")
    
    console.print()


def main():
    """Entry point for CLI."""
    app()


if __name__ == "__main__":
    main()
