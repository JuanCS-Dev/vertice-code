"""
ReviewerAgent v3.0: Production-Grade Symbolic Auditor.

Implements real McCabian Cyclomatic Complexity analysis and AST-based
variable tracking to feed the LLM with concrete metrics, not guesses.

Capabilities:
    - Real Cyclomatic Complexity Calculation (AST traversal)
    - Scope Analysis (Variable tracking)
    - Hybrid LLM/Static Review

Architecture:
    ReviewerAgent (BaseAgent)
        └── SymbolicVisitor (AST NodeVisitor) -> Extracts concrete metrics
"""

import ast
import json
from typing import Any, Dict, List, Optional, Set, Tuple
from pydantic import BaseModel, Field

from .base import (
    AgentCapability,
    AgentResponse,
    AgentRole,
    AgentTask,
    BaseAgent,
)

# --- Production Data Models ---

class ComplexityMetric(BaseModel):
    function_name: str
    complexity: int # McCabe index
    loc: int # Logical lines of code
    args_count: int
    returns_count: int

class CodeIssue(BaseModel):
    file: str
    line: int
    severity: str # CRITICAL, HIGH, MEDIUM, LOW
    category: str
    message: str
    fix: Optional[str] = None

class ReviewReport(BaseModel):
    approved: bool
    score: int
    metrics: List[ComplexityMetric]
    issues: List[CodeIssue]
    summary: str

# --- The Heavy Lifting: Real Static Analysis ---

class SymbolicVisitor(ast.NodeVisitor):
    """
    Production-grade AST Visitor.
    Calculates real metrics used by linters like Flake8/Radon.
    """
    def __init__(self):
        self.metrics: List[ComplexityMetric] = []
        self.current_function: Optional[str] = None
        self.complexity = 0
        self.loc = 0
        self.returns = 0
        self.args = 0
        
    def visit_FunctionDef(self, node: ast.FunctionDef):
        self._enter_function(node)
        self.generic_visit(node)
        self._exit_function()

    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):
        self._enter_function(node)
        self.generic_visit(node)
        self._exit_function()

    def _enter_function(self, node):
        self.current_function = node.name
        self.complexity = 1 # Base complexity
        self.loc = 0
        self.returns = 0
        self.args = len(node.args.args)

    def _exit_function(self):
        if self.current_function:
            self.metrics.append(ComplexityMetric(
                function_name=self.current_function,
                complexity=self.complexity,
                loc=self.loc,
                args_count=self.args,
                returns_count=self.returns
            ))
            self.current_function = None

    # Cyclomatic Complexity Logic
    def visit_If(self, node):
        self.complexity += 1
        self.generic_visit(node)

    def visit_For(self, node):
        self.complexity += 1
        self.generic_visit(node)

    def visit_AsyncFor(self, node):
        self.complexity += 1
        self.generic_visit(node)

    def visit_While(self, node):
        self.complexity += 1
        self.generic_visit(node)

    def visit_Try(self, node):
        self.complexity += len(node.handlers)
        self.generic_visit(node)

    def visit_BoolOp(self, node):
        # 'and'/'or' add to complexity path
        self.complexity += len(node.values) - 1
        self.generic_visit(node)
        
    def visit_Return(self, node):
        self.returns += 1
        self.generic_visit(node)

    def visit_expr(self, node):
        self.loc += 1
        self.generic_visit(node)

# --- The Agent ---

class ReviewerAgent(BaseAgent):
    """
    The Auditor. Uses Real Static Analysis to guide Neural Inference.
    """
    
    def __init__(self, llm_client: Any, mcp_client: Any):
        super().__init__(
            role=AgentRole.REVIEWER,
            capabilities=[AgentCapability.READ_ONLY],
            llm_client=llm_client,
            mcp_client=mcp_client,
            system_prompt=self._build_system_prompt()
        )

    def _build_system_prompt(self) -> str:
        return """
        You are ReviewerAgent v3.0 (Production).
        
        INPUT: Code + Static Analysis Metrics (Cyclomatic Complexity).
        TASK: Semantic & Logic Review.
        
        RULES:
        1. If Complexity > 10: Demands refactoring.
        2. If Args > 5: Suggest Data Class.
        3. Look for Logic bugs that static analysis misses (e.g., race conditions, business logic flaws).
        
        Output strict JSON matching: {"issues": [...], "summary": "..."}
        """

    async def execute(self, task: AgentTask) -> AgentResponse:
        try:
            files_map = await self._load_context(task)
            if not files_map:
                return AgentResponse(
                    success=False, 
                    reasoning="No files found to review.",
                    error="No files provided"
                )

            all_metrics = []
            all_issues = []
            
            # Phase 1: Hard Analysis (Deterministic)
            for fname, content in files_map.items():
                if not fname.endswith('.py'):
                    continue
                    
                try:
                    tree = ast.parse(content)
                    visitor = SymbolicVisitor()
                    visitor.visit(tree)
                    all_metrics.extend(visitor.metrics)
                    
                    # Heuristic Checks (Immediate Failures)
                    for m in visitor.metrics:
                        if m.complexity > 15:
                            all_issues.append(CodeIssue(
                                file=fname, line=1, severity="HIGH", category="COMPLEXITY",
                                message=f"Function '{m.function_name}' has Cyclomatic Complexity {m.complexity} (Max 15). Refactor needed.",
                                fix=f"Break down '{m.function_name}' into smaller functions"
                            ))
                        if m.args_count > 5:
                            all_issues.append(CodeIssue(
                                file=fname, line=1, severity="MEDIUM", category="DESIGN",
                                message=f"Function '{m.function_name}' has {m.args_count} arguments. Consider using a dataclass.",
                                fix="Create a parameter object/dataclass"
                            ))
                            
                except SyntaxError as e:
                    all_issues.append(CodeIssue(
                        file=fname, line=e.lineno or 0, severity="CRITICAL", category="SYNTAX",
                        message=f"Syntax Error: {e.msg}"
                    ))

            # Phase 2: Neural Analysis (LLM) - Only if no syntax errors
            critical_syntax = [i for i in all_issues if i.category == "SYNTAX"]
            llm_summary = "Static analysis complete."
            
            if not critical_syntax:
                prompt_context = {
                    "static_metrics": [m.model_dump() for m in all_metrics],
                    "code_samples": {k: v[:5000] for k, v in files_map.items()}
                }
                
                prompt = f"""
                Analyze this code context for LOGIC and SECURITY issues.
                
                METRICS: {json.dumps(prompt_context['static_metrics'], indent=2)}
                
                Focus on:
                - Race conditions
                - Edge cases not handled
                - Security vulnerabilities (injection, auth bypass)
                - Business logic flaws
                
                Respond with JSON: {{"issues": [{{"file": "...", "line": 1, "severity": "HIGH", "category": "SECURITY", "message": "..."}}], "summary": "..."}}
                """
                
                try:
                    llm_raw = await self._call_llm(prompt, temperature=0.2)
                    llm_report = self._parse_llm_json(llm_raw)
                    
                    # Merge LLM issues
                    llm_issues = llm_report.get("issues", [])
                    for issue_dict in llm_issues:
                        if isinstance(issue_dict, dict):
                            all_issues.append(CodeIssue(**issue_dict))
                    
                    llm_summary = llm_report.get("summary", llm_summary)
                except Exception as e:
                    llm_summary = f"LLM analysis failed: {str(e)}"
            
            # Final Scoring
            final_score = self._calculate_score(all_issues, all_metrics)
            
            report = ReviewReport(
                approved=final_score >= 80 and len([i for i in all_issues if i.severity == "CRITICAL"]) == 0,
                score=final_score,
                metrics=all_metrics,
                issues=all_issues,
                summary=llm_summary
            )
            
            return AgentResponse(
                success=True,
                data={"report": report.model_dump()},
                reasoning=f"Analyzed {len(all_metrics)} functions. Score: {final_score}/100. Issues: {len(all_issues)}."
            )

        except Exception as e:
            return AgentResponse(
                success=False, 
                error=str(e), 
                reasoning=f"Review crashed: {str(e)}"
            )

    async def _load_context(self, task: AgentTask) -> Dict[str, str]:
        files = task.context.get("files", [])
        contents = {}
        for f in files:
            try:
                res = await self._execute_tool("read_file", {"path": f})
                if res.get("success"):
                    contents[f] = res.get("content", "")
                else:
                    # Fallback: direct file read
                    with open(f, 'r', encoding='utf-8') as file:
                        contents[f] = file.read()
            except Exception:
                continue
        return contents

    def _parse_llm_json(self, text: str) -> Dict[str, Any]:
        """Robust JSON extraction."""
        import re
        try:
            # Try finding JSON object
            match = re.search(r'\{.*\}', text, re.DOTALL)
            if match:
                return json.loads(match.group(0))
            return {"issues": [], "summary": "LLM response parsing failed"}
        except:
            return {"issues": [], "summary": "LLM response invalid JSON"}

    def _calculate_score(self, issues: List[CodeIssue], metrics: List[ComplexityMetric]) -> int:
        score = 100
        
        # Deduction for Issues
        severity_map = {"CRITICAL": 25, "HIGH": 10, "MEDIUM": 5, "LOW": 2}
        for i in issues:
            score -= severity_map.get(i.severity.upper(), 1)
        
        # Deduction for average complexity
        if metrics:
            avg_complex = sum(m.complexity for m in metrics) / len(metrics)
            if avg_complex > 8:
                score -= int((avg_complex - 8) * 2)
                
        return max(0, min(100, score))
