# üî¨ Relat√≥rio Cient√≠fico ‚Äî Otimiza√ß√£o de Performance (Textual TUI) ‚Äî Vertice-Code

**Data:** 2026-01-19
**Escopo:** performance (lat√™ncia, smoothness de streaming, mem√≥ria/CPU em sess√µes longas) do **Textual TUI**.
**Base de padr√µes:** `PHASE_1_REPORT.md` (Textual 2026 patterns + URLs oficiais).

---

## Status desta sess√£o (aplicado no repo)

‚úÖ P0 implementado (hot paths) + higiene de testes:
- Autocomplete: debounce + cancelamento via `run_worker(... exclusive=True)`: `src/vertice_tui/app.py:402`
- Autocomplete: dropdown sem churn (reuso de `MAX_ITEMS` widgets + `batch_update()`): `src/vertice_tui/widgets/autocomplete.py:17`
- Autocomplete: `asyncio.to_thread(...)` + wrapper thread-safe: `src/vertice_tui/core/ui_bridge.py:537`
- StatusBar: cria `#tokens` e evita watchers antes de mount: `src/vertice_tui/widgets/status_bar.py:124`
- pytest-asyncio: remove redefini√ß√£o de `event_loop` (zera DeprecationWarning): `tests/conftest.py:1`
- E2E: fixture em formato *factory async context manager* (cura ContextVar teardown): `tests/e2e/conftest.py:221`

‚úÖ P1 implementado (smoothness de streaming):
- Streaming: coalescing de deltas por ‚Äúframe‚Äù (flush peri√≥dico + `SoftBuffer` + scroll throttled): `src/vertice_tui/widgets/response_view.py:135`
- Config: `VERTICE_TUI_STREAM_FLUSH_MS` (default 33ms; min 5ms)

Provas (testes adicionados/atualizados):
- Dropdown n√£o monta/desmonta por tecla: `tests/integration/test_tui_performance.py:162`
- Streaming: m√∫ltiplos deltas ‚Üí 1 write coalescido (determin√≠stico): `tests/integration/test_tui_performance.py:210`
- `run_test()` teardown est√°vel (anti ContextVar): `tests/e2e/test_run_test_contextvars.py:1`

Valida√ß√£o executada (sess√£o atual):
- `black src/vertice_tui/widgets/response_view.py src/vertice_tui/core/streaming/soft_buffer.py tests/integration/test_tui_performance.py`
- `ruff check src/vertice_tui/widgets/response_view.py src/vertice_tui/core/streaming/soft_buffer.py tests/integration/test_tui_performance.py`
- `pytest -v tests/integration/test_tui_performance.py -k ResponseViewStreamingCoalescing -x`
- `pytest -v tests/e2e/test_basics.py -x`

---

## 0) Baseline (evitar ‚Äúdoc drift‚Äù)

- Textual instalado (repo): `6.2.1` (ver `PHASE_1_REPORT.md`)
- pytest-asyncio instalado: `0.24.0` (ver `PHASE_1_REPORT.md`)
- Config pytest do repo: `pytest.ini` cont√©m `asyncio_mode=auto` e `asyncio_default_fixture_loop_scope=function` (ver `PHASE_1_REPORT.md`)

Fontes oficiais usadas nesta auditoria:
- Workers: `https://textual.textualize.io/guide/workers/` e `https://textual.textualize.io/api/work/`
- Reactivity: `https://textual.textualize.io/guide/reactivity/`
- Lazy: `https://textual.textualize.io/api/lazy/`
- Testing (run_test/Pilot): `https://textual.textualize.io/guide/testing/#testing-apps`
- asyncio.to_thread: `https://docs.python.org/3/library/asyncio-task.html#asyncio.to_thread`

---

## 1) Arquitetura atual (caminhos cr√≠ticos)

### 1.1 App / Input / Workers
- `src/vertice_tui/app.py`
  - `VerticeApp.compose()` define √°rvore base (‚úÖ conforme padr√£o de lifecycle).
  - `VerticeApp.on_input_submitted()` usa `run_worker(... group="chat_dispatch", exclusive=True)` para chat e slash commands (‚úÖ conforme Workers).
  - `VerticeApp.on_input_changed()` calcula autocomplete em *cada* mudan√ßa do input (hot path de digita√ß√£o).

### 1.2 Streaming ‚Üí UI incremental
- `src/vertice_tui/app.py` (`_handle_chat`)
  - `async for sse_chunk in self.bridge.chat(message)` e, para cada `OutputTextDelta`, chama `await ResponseView.append_chunk(delta)`
- `src/vertice_tui/widgets/response_view.py`
  - `TextualMarkdown.get_stream()` (MarkdownStream) para streaming incremental
  - coalescing: deltas s√£o bufferizados e flushados em cad√™ncia fixa (default 33ms) via timer
  - config: `VERTICE_TUI_STREAM_FLUSH_MS` (ms, m√≠nimo 5ms)
  - throttling de `scroll_end()` (50ms) para reduzir layout thrash
  - limite de scrollback (`VERTICE_TUI_MAX_VIEW_ITEMS`, default 300) para evitar crescimento infinito

### 1.3 Providers (impactam ‚Äútime to first token‚Äù)
- `src/vertice_cli/core/providers/vertex_ai.py`
  - `VertexAIProvider._stream_v3(...)` usa API async do `google-genai` (n√£o deve bloquear o loop se usada corretamente)

---

## 2) O que j√° est√° forte (padr√µes Textual 2026 aplicados)

### 2.1 Workers usados corretamente no caminho cr√≠tico
‚úÖ `run_worker(... group="chat_dispatch", exclusive=True)` em `src/vertice_tui/app.py` reduz travas do handler e permite cancelamento determin√≠stico.
Fonte: `https://textual.textualize.io/guide/workers/` + `https://textual.textualize.io/api/work/`

### 2.2 Streaming incremental correto (evita ‚Äúrepaint‚Äù gigante)
‚úÖ `TextualMarkdown.get_stream()` em `src/vertice_tui/widgets/response_view.py` evita re-renderizar o texto inteiro a cada delta.
Fonte: `https://textual.textualize.io/guide/widgets/`

### 2.3 Growth control / sess√µes longas
‚úÖ `ResponseView._trim_view_items()` limita widgets (default 300). Isso evita degrada√ß√£o por acumular milhares de widgets.

### 2.4 I/O de hist√≥rico j√° tem trilha async
‚úÖ `HistoryManager.add_command_async()` usa `asyncio.to_thread(...)` e lock para n√£o bloquear o loop.
Fonte: `https://docs.python.org/3/library/asyncio-task.html#asyncio.to_thread`

---

## 3) Achados de performance (n√£o-conformidades / gargalos)

> Severidade = impacto esperado em lat√™ncia percebida, FPS e estabilidade em sess√µes longas.

### A) Autocomplete: churn de widgets + computa√ß√£o s√≠ncrona no hot path (SEV: HIGH)

**Onde:**
- `src/vertice_tui/app.py` ‚Üí `VerticeApp.on_input_changed()` (chamado a cada tecla)
- `src/vertice_tui/widgets/autocomplete.py` ‚Üí `AutocompleteDropdown.show_completions()` remove e remonta at√© 15 widgets sempre
- `src/vertice_tui/core/ui_bridge.py` ‚Üí `AutocompleteBridge.get_completions()` pode fazer scan (`Path.cwd().iterdir()`) no primeiro uso de `@...`

**Por que importa:**
- Remover/montar widgets repetidamente for√ßa layout + diff do DOM do Textual.
- Computa√ß√£o s√≠ncrona em handler de digita√ß√£o reduz ‚Äútyping FPS‚Äù e causa input lag (pior em repos grandes).

**Sinais no c√≥digo:**
- `show_completions()` remove todos widgets e remonta (custo = O(n) por tecla).
- `query_one(...)` dentro de handlers de alta frequ√™ncia (digita√ß√£o/teclas).

---

### B) Streaming: excesso de writes pequenos em MarkdownStream (SEV: MED) ‚Äî CURADO ‚úÖ

**Onde:** `src/vertice_tui/widgets/response_view.py` (`append_chunk` + `_flush_pending_stream_async`)
**Por que importava:** em taxas altas de deltas (tokens/seg), muitos `await MarkdownStream.write(...)` pequenos aumentam overhead de render/measure.
**Cura aplicada:** `append_chunk()` agora apenas bufferiza deltas e o flush √© coalescido por ‚Äúframe‚Äù (timer), com `SoftBuffer` para evitar jank por markdown incompleto.

**Como funciona agora (resumo):**
- Buffer: `self._pending_stream_chunks: list[str]`
- Flush: `self.set_interval(..., self._flush_tick)` + `await MarkdownStream.write(coalesced)`
- Safety: `SoftBuffer` mant√©m tokens markdown ‚Äúincompletos‚Äù fora da tela at√© ficar seguro (ou finaliza√ß√£o).

**Config/tuning:**
- `VERTICE_TUI_STREAM_FLUSH_MS` default `33` (30fps). Para ‚Äúmais responsivo‚Äù, testar `16`; para ‚Äúmenos CPU‚Äù, testar `50‚Äì75`.

**Prova (sem timers):**
- `tests/integration/test_tui_performance.py:210`

---

### C) Renderables caros em scrollback (SEV: MED/HIGH em sess√µes longas)

**Onde:** `src/vertice_tui/widgets/response_view.py`
**Por que importa:** mesmo com limite de 300 itens, alguns widgets s√£o caros (Rich `Panel`, `Syntax` com `line_numbers=True` + `word_wrap=True`). Em sess√µes longas com muito c√≥digo/diff, 300 pain√©is ainda podem ser pesados para scroll e layout.

---

### D) Exce√ß√µes em watchers no StatusBar (SEV: LOW/MED, mas ‚Äúdeath by a thousand cuts‚Äù)

**Onde:** `src/vertice_tui/widgets/status_bar.py`
**Sintoma:** watchers de `token_used/token_limit` atualizam `#tokens`, mas `compose()` n√£o cria um `Static(id="tokens")`. Isso gera exce√ß√£o (capturada) e logging debug pode virar custo constante se tokens forem atualizados frequentemente.

---

## 4) Recomenda√ß√µes (priorizadas, com prova cient√≠fica)

### P0 ‚Äî ROI alto, baixo risco (fa√ßa primeiro)

1) **Debounce + cancelamento do autocomplete**
   - Objetivo: o handler de digita√ß√£o s√≥ agenda trabalho; c√°lculo roda em worker (thread) com `exclusive=True` e cancela o anterior.
   - Padr√£o: Workers (Textual) + ‚Äúno blocking work in handlers‚Äù.
   - Fonte: `https://textual.textualize.io/guide/workers/` e `https://textual.textualize.io/api/work/`

2) **Reduzir churn no dropdown**
   - Trocar ‚Äúremove + mount‚Äù por ‚Äúreuso + update‚Äù (manter 15 `Static` fixos e atualizar texto/classes).
   - Usar `batch_update()` ao atualizar v√°rias linhas de uma vez (reduz layout thrash).

3) **Cache de widgets consultados com frequ√™ncia**
   - Guardar refer√™ncias em `on_mount()` (`self._prompt`, `self._autocomplete`, `self._response`, `self._status`) para evitar `query_one(...)` em hot paths.
   - O ganho √© pequeno por chamada, mas grande no agregado (digita√ß√£o + streaming).

4) **Corrigir o `StatusBar` para n√£o gerar exce√ß√µes em watchers**
   - Ou criar `Static(id="tokens")` e usar `_format_tokens()`, ou remover o update de `#tokens` (se o MiniTokenMeter √© a √∫nica UI).

5) **Eliminar `query_one(...)` remanescente em caminhos de alta frequ√™ncia**
   - Ex.: `VerticeApp.on_input_submitted()` e a√ß√µes de scroll ainda fazem m√∫ltiplos `query_one("#response")`/`query_one(StatusBar)` por evento.
   - Sugest√£o: usar as refer√™ncias cacheadas em `on_mount()` e, quando necess√°rio, validar `is_mounted` antes de atualizar.
   - Fonte (lifecycle + handlers): `https://textual.textualize.io/guide/app/` e `https://textual.textualize.io/guide/events/`

6) **HUDs/Widgets com updates frequentes: cache dos children + atualiza√ß√£o minimalista**
   - `PerformanceHUD._update_display()` faz 4√ó `query_one(...).update(...)` por update; se isso rodar por token/frame vira custo constante.
   - Sugest√£o: capturar refs em `on_mount()` (`self._latency_widget`, etc.), e aplicar um ‚Äúupdate only if changed‚Äù.
   - Fonte (reactivity: ‚Äúwatchers leves‚Äù): `https://textual.textualize.io/guide/reactivity/`

**Prova (testes/bench):**
- Adicionar/rodar microbenchmark de digita√ß√£o: simular 200 `Input.Changed` em < X ms sob run_test.
- M√©trica: P95 do tempo de handler < 2ms; UI responsiva.

---

### P1 ‚Äî Streaming/render: suavidade e CPU

7) **Coalescing de deltas por frame**
   - Bufferizar deltas recebidos e dar flush a cada ~16‚Äì33ms (60‚Äì30fps), ao inv√©s de `write()` por delta.
   - Integrar `SoftBuffer` (j√° existe em `src/vertice_tui/core/streaming/soft_buffer.py`) para flush em fronteiras ‚Äúseguras‚Äù de Markdown.
   - Implementado em `ResponseView` com timer + `SoftBuffer` + scroll throttled.
   - Resultado esperado: menos overhead de render/measure, mesmo throughput visual.

8) **Evitar O(n¬≤) em concat de `current_response`**
   - Durante streaming com `MarkdownStream`, armazenar em `list[str]` e s√≥ `''.join(...)` no final (ou sob demanda).
   - Mant√©m compatibilidade (export/copy) sem custo por token.

9) **Streaming ‚Äúflush worker‚Äù dedicado (sem work no handler de evento)**
   - Hoje cada `OutputTextDeltaEvent` chama `append_chunk()`; isso pode virar milhares de awaits curtos.
   - Sugest√£o: o handler s√≥ faz `buffer.append(delta)` e agenda/garante um √∫nico worker exclusivo que d√° flush periodicamente (30‚Äì60fps) e faz `scroll_end()` no flush.
   - Fonte (Workers + cancelamento determin√≠stico): `https://textual.textualize.io/guide/workers/`

10) **Renderables caros: aplicar lazy/limits e ‚Äúdegrada√ß√£o controlada‚Äù**
   - `Panel(Syntax(... line_numbers=True, word_wrap=True))` em blocos grandes tende a custar caro em scroll/render.
   - Sugest√£o: limites por env/config (ex.: `MAX_CODE_LINES`, `MAX_DIFF_LINES`) + bot√£o/a√ß√£o ‚Äúexpand‚Äù usando `Lazy(...)` para materializar s√≥ quando necess√°rio.
   - Fonte: `https://textual.textualize.io/api/lazy/`

**Prova (testes/bench):**
- Benchmark determin√≠stico (sem rede): gerar 10k deltas e medir tempo total + n√∫mero de flushes.
- M√©trica: reduzir `writes/s` e reduzir CPU por delta mantendo tempo total semelhante.

---

### P2 ‚Äî Sess√µes longas: mem√≥ria e scroll

11) **‚ÄúCompaction‚Äù de scrollback: degradar renderables antigos para formato barato**
   - Quando `ResponseView` exceder limite, substituir blocos antigos por um ‚ÄúSummaryStatic‚Äù simples (texto plano / markdown j√° consolidado), ou migrar hist√≥rico antigo para `TextLog`.
   - Objetivo: manter UX do ‚Äúrecent history‚Äù rica, mas evitar que *tudo* seja `Panel/Syntax`.

12) **Limites expl√≠citos para blocos caros**
   - Ex.: limitar `code-block` a N linhas por padr√£o (config/env), com a√ß√£o ‚Äúexpand‚Äù (lazy).
   - Padr√£o: `Lazy(...)` para materializar widget pesado s√≥ quando necess√°rio.
   - Fonte: `https://textual.textualize.io/api/lazy/`

13) **Warm-up de caches em background (autocomplete + filesystem)**
   - O primeiro `@...` pode disparar varredura (mesmo limitada) e causar micro-lag.
   - Sugest√£o: agendar worker (thread) no `on_mount()` para preencher cache de arquivos em idle, e manter invalidation quando `cwd`/root mudar.
   - Fonte: `https://textual.textualize.io/guide/workers/`

**Prova (testes/bench):**
- Teste de estabilidade de mem√≥ria com `tracemalloc` (ou `memray` quando dispon√≠vel).
- M√©trica: crescimento sublinear em 10k mensagens (sem leaks).

---

## 5) Plano de instrumenta√ß√£o (o ‚Äúcient√≠fico‚Äù)

### 5.1 M√©tricas m√≠nimas (KPIs)
- **Enter‚ÜíUI feedback (ms):** tempo at√© a mensagem do usu√°rio aparecer no `ResponseView`.
- **Enter‚Üí1¬∫ delta (ms):** j√° existe via JSONL (`VERTICE_TUI_PERF_LOG_PATH`).
- **Writes/s no markdown stream:** n√∫mero de flushes/seg (antes/depois do coalescing).
- **CPU por 1k deltas:** `time.process_time()` ou profiling externo.
- **Mem√≥ria:** `tracemalloc` ou `memray` em cen√°rios de 10k mensagens.

### 5.2 Harness recomendado
- Preferir `run_test()` + `Pilot` para benchmarks determin√≠sticos.
  - Fonte: `https://textual.textualize.io/guide/testing/#testing-apps`
- Reaproveitar infra existente:
  - `tests/integration/test_tui_performance.py` (hist√≥rico async + streaming async)
  - `tests/e2e/test_run_test_contextvars.py` (estabilidade de teardown)

---

## 6) Checklist ‚ÄúDone = provado‚Äù

- [x] Autocomplete n√£o bloqueia digita√ß√£o (debounce + worker `exclusive=True`) ‚Äî `src/vertice_tui/app.py:382`
- [x] Autocomplete sem churn de widgets (reuso de children) ‚Äî `tests/integration/test_tui_performance.py:162`
- [x] Streaming mant√©m UX, mas reduz writes/s (coalescing) com ganho de CPU ‚Äî `src/vertice_tui/widgets/response_view.py:135`
- [ ] Sess√£o longa (10k mensagens) n√£o degrada scroll de forma exponencial (mem√≥ria controlada, sem leaks).
- [ ] Testes de performance passam em CI sem rede (mocks determin√≠sticos).

---

## 7) Pr√≥ximas a√ß√µes (sequ√™ncia sugerida)

1) Completar P1 (itens 8‚Äì10: evitar O(n¬≤), flush worker dedicado, lazy/limits)
2) Implementar P2 (itens 11‚Äì13: compaction + warm caches)
3) Consolidar benchmarks em `tests/integration/` e rodar continuamente
