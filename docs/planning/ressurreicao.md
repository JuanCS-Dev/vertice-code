# üî• Plano de Ressurrei√ß√£o do Neuroshell CLI

**Objetivo**: Transformar o shell interativo em um CLI t√£o bom quanto Gemini e Claude Code, com startup instant√¢neo, respostas r√°pidas e experi√™ncia fluida.

---

## üìä An√°lise da Situa√ß√£o Atual

### Problemas Identificados

1. **Startup Lento (10s)**
   - `indexer.load_cache()` bloqueando no `__init__`
   - Inicializa√ß√£o s√≠ncrona de 15+ componentes pesados
   - LSP Client, RefactoringEngine, ContextSuggestionEngine carregados antes do uso

2. **Shell Travando ao Digitar**
   - `enhanced_input.prompt_async()` com problemas
   - Workflow visualizer rodando em foreground
   - File watcher fazendo polling s√≠ncrono

3. **Processamento LLM Lento**
   - M√©todo `_process_request_with_llm` com 300+ linhas
   - M√∫ltiplas camadas de abstra√ß√£o desnecess√°rias
   - Confirma√ß√µes de seguran√ßa excessivas para comandos seguros

4. **Arquitetura Complexa Demais**
   - Shell.py com 2479 linhas e 50 m√©todos
   - M√∫ltiplos sistemas de contexto sobrepostos
   - Depend√™ncias circulares entre componentes

### Pontos Fortes (Manter)

‚úÖ **27 Tools Funcionais** - Bem estruturadas e testadas
‚úÖ **DevSquad Orchestration** - Sistema de agentes robusto
‚úÖ **MCP Integration** - Protocolo moderno de contexto
‚úÖ **Rich TUI Components** - Interface visual bem feita
‚úÖ **Circuit Breaker & Rate Limiting** - LLM client resiliente

---

## üéØ Arquitetura Alvo (Inspirada em Claude Code + Gemini)

### Princ√≠pios de Design

1. **Lazy Loading** - Carregar componentes s√≥ quando necess√°rio
2. **Async First** - Tudo ass√≠ncrono, sem bloqueios
3. **Streaming** - Respostas incrementais do LLM
4. **Minimal UI** - Interface limpa e r√°pida
5. **Tool-First** - Foco nas tools, n√£o na orquestra√ß√£o

### Fluxo Simplificado

```
User Input ‚Üí LLM (streaming) ‚Üí Tool Calls ‚Üí Execute ‚Üí Stream Results
     ‚Üì                                              ‚Üì
  Minimal UI                              Background Tasks
```

---

## üîß Plano de Implementa√ß√£o (5 Fases)

### **FASE 1: Startup Instant√¢neo** (Prioridade CR√çTICA)

**Objetivo**: Shell inicializa em < 1s

#### 1.1 Lazy Initialization Pattern
```python
class InteractiveShell:
    def __init__(self):
        # APENAS essenciais
        self.console = Console()
        self.llm = default_llm_client
        self.registry = ToolRegistry()
        self._register_tools()  # R√°pido

        # Lazy properties
        self._indexer = None
        self._lsp_client = None
        self._squad = None

    @property
    def indexer(self):
        if self._indexer is None:
            self._indexer = SemanticIndexer()
            asyncio.create_task(self._indexer.load_cache())
        return self._indexer
```

#### 1.2 Background Loading
- Mover `load_cache()` para task ass√≠ncrona
- LSP, Refactoring Engine, Context Suggestions ‚Üí lazy
- Dashboard, Animations, Workflow Viz ‚Üí lazy

#### 1.3 Remover Inicializa√ß√µes Duplicadas
- `RichContextBuilder` aparece 2x (linhas 149 e 153)
- Consolidar em um √∫nico sistema de contexto

**Resultado Esperado**: `neuroshell-code` inicia em 0.5-1s

---

### **FASE 2: Input Responsivo** (Prioridade ALTA)

**Objetivo**: Digitar n√£o trava, resposta imediata

#### 2.1 Simplificar Input Loop
```python
async def run(self):
    self._show_welcome()

    while True:
        try:
            # Input simples, sem enhanced_input complexo
            user_input = await asyncio.to_thread(
                self.console.input, "‚ùØ "
            )

            if not user_input.strip():
                continue

            # Process async, n√£o bloqueia
            asyncio.create_task(
                self._process_input(user_input)
            )

        except KeyboardInterrupt:
            break
```

#### 2.2 Remover Workflow Visualizer do Foreground
- Mover para background task opcional
- S√≥ ativar com flag `--verbose`

#### 2.3 Otimizar File Watcher
- Aumentar intervalo de polling para 5s
- Usar `asyncio.sleep()` em vez de loop s√≠ncrono

**Resultado Esperado**: Input nunca trava, shell sempre responsivo

---

### **FASE 3: LLM Streaming R√°pido** (Prioridade ALTA)

**Objetivo**: Respostas aparecem incrementalmente, como Claude Code

#### 3.1 Implementar Streaming Real
```python
async def _process_input_streaming(self, user_input: str):
    # Minimal loading indicator
    self.console.print("[dim]...[/dim]", end="", flush=True)

    # Stream LLM response
    full_response = ""
    async for chunk in self.llm.stream_chat(
        prompt=user_input,
        system_prompt=self._get_system_prompt()
    ):
        full_response += chunk
        # Update display incrementally
        self.console.print(f"\r{chunk}", end="", flush=True)

    # Parse tool calls from complete response
    tool_calls = self._parse_tool_calls(full_response)
    if tool_calls:
        await self._execute_tools_fast(tool_calls)
```

#### 3.2 Usar `stream_chat` do LLMClient
- J√° implementado em `llm.py` (linha 277)
- Suporta failover e circuit breaker
- Retorna chunks incrementais

#### 3.3 Remover Camadas Desnecess√°rias
- Deletar `_process_request_with_llm` antigo (300 linhas)
- Usar abordagem direta do `single_shot.py`
- Sem confirma√ß√µes para comandos READ-ONLY

**Resultado Esperado**: Respostas aparecem em < 0.5s, streaming vis√≠vel

---

### **FASE 4: Simplificar Arquitetura** (Prioridade M√âDIA)

**Objetivo**: C√≥digo mais limpo, f√°cil de manter

#### 4.1 Refatorar Shell.py
```
Atual: 2479 linhas, 50 m√©todos
Alvo:  < 800 linhas, 20 m√©todos principais
```

**Extrair para M√≥dulos**:
- `shell_core.py` - Loop principal, input, output
- `shell_llm.py` - Processamento LLM e streaming
- `shell_tools.py` - Execu√ß√£o de tools
- `shell_commands.py` - Comandos do sistema (/help, /metrics)

#### 4.2 Consolidar Sistemas de Contexto
- Remover `ContextBuilder`, `RichContextBuilder`, `ConsolidatedContextManager`
- Manter apenas `ContextAwarenessEngine`
- Simplificar para 1 sistema √∫nico

#### 4.3 Remover Features N√£o Usadas
- Command Palette (Ctrl+K) ‚Üí Raramente usado
- Animations ‚Üí Desnecess√°rio para CLI
- Dashboard ‚Üí Mover para comando `/dashboard`

**Resultado Esperado**: C√≥digo 60% menor, mais f√°cil debugar

---

### **FASE 5: Otimiza√ß√µes Finais** (Prioridade BAIXA)

**Objetivo**: Performance de produ√ß√£o

#### 5.1 Caching Inteligente
```python
from functools import lru_cache

@lru_cache(maxsize=128)
def _get_system_prompt(self) -> str:
    # Cache prompt generation
    return self._build_system_prompt()
```

#### 5.2 Tool Execution Paralela
```python
async def _execute_tools_parallel(self, tool_calls):
    tasks = [
        self._execute_single_tool(call)
        for call in tool_calls
    ]
    return await asyncio.gather(*tasks)
```

#### 5.3 Metrics & Monitoring
- Adicionar timing logs (opcional, `--debug`)
- Track tool usage statistics
- Monitor LLM token consumption

**Resultado Esperado**: Shell 2x mais r√°pido que vers√£o atual

---

## üìã Checklist de Implementa√ß√£o

### Fase 1: Startup Instant√¢neo
- [ ] Implementar lazy loading para indexer
- [ ] Mover LSP/Refactoring para properties lazy
- [ ] Background task para load_cache()
- [ ] Remover inicializa√ß√µes duplicadas
- [ ] Testar: `time neuroshell-code --help` < 1s

### Fase 2: Input Responsivo
- [ ] Simplificar loop principal do `run()`
- [ ] Remover `enhanced_input` complexo
- [ ] Workflow viz ‚Üí background opcional
- [ ] File watcher ‚Üí async com 5s interval
- [ ] Testar: digitar n√£o trava

### Fase 3: LLM Streaming
- [ ] Implementar `_process_input_streaming()`
- [ ] Integrar `llm.stream_chat()`
- [ ] Remover `_process_request_with_llm` antigo
- [ ] Streaming incremental no console
- [ ] Testar: resposta aparece em < 0.5s

### Fase 4: Simplificar Arquitetura
- [ ] Extrair `shell_core.py`
- [ ] Extrair `shell_llm.py`
- [ ] Extrair `shell_tools.py`
- [ ] Consolidar sistemas de contexto
- [ ] Remover features n√£o usadas

### Fase 5: Otimiza√ß√µes
- [ ] Adicionar caching com `lru_cache`
- [ ] Tool execution paralela
- [ ] Metrics opcionais (`--debug`)
- [ ] Benchmark completo

---

## üéØ M√©tricas de Sucesso

| M√©trica | Atual | Alvo | Como Medir |
|---------|-------|------|------------|
| **Startup Time** | 10s | < 1s | `time neuroshell-code --help` |
| **First Response** | 3-5s | < 0.5s | Tempo at√© primeiro chunk LLM |
| **Input Lag** | Trava | 0ms | Digitar nunca bloqueia |
| **Memory Usage** | ? | < 200MB | `ps aux \| grep neuroshell` |
| **Code Size** | 2479 linhas | < 800 linhas | `wc -l shell.py` |

---

## üöÄ Pr√≥ximos Passos Imediatos

1. **Come√ßar pela Fase 1** - Startup √© o problema mais cr√≠tico
2. **Testar incrementalmente** - Cada mudan√ßa deve ser test√°vel
3. **Manter tools funcionando** - N√£o quebrar as 27 tools existentes
4. **Documentar mudan√ßas** - Atualizar este plano conforme progresso

---

## üìö Refer√™ncias Pesquisadas

### Anthropic Claude SDK
- **Async Streaming**: `client.messages.stream()` com `text_stream`
- **Best Practice**: Usar `async for` para chunks incrementais
- **Error Handling**: Circuit breaker + exponential backoff

### Claude Code Architecture
- **Client-Server**: CLI local + remote AI model
- **MCP Protocol**: Extensibilidade via Model Context Protocol
- **Tool Calls**: Permiss√µes granulares, allowlist customiz√°vel
- **React-in-Terminal**: Ink + Yoga para UI din√¢mica

### Gemini CLI Patterns
- **Stream JSON**: `--output-format stream-json` para eventos
- **HTTP Streaming**: Endpoints para dados cont√≠nuos
- **MCP Extensions**: Servidores custom para funcionalidades

---

**√öltima Atualiza√ß√£o**: 2025-11-22
**Status**: Plano aprovado, pronto para execu√ß√£o
**Pr√≥xima Revis√£o**: Ap√≥s Fase 1 completa
