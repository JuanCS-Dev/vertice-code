# STREAMING AUDIT REPORT - MAESTRO UI v10.0

**Date**: 2025-11-24
**Auditor**: Claude Code (Constitutional Analysis Mode)
**Issue**: Agent streaming not displaying in real-time in MAESTRO UI
**Severity**: üî¥ **CRITICAL** - Core feature non-functional

---

## EXECUTIVE SUMMARY

**Problem Observed**: Screenshot shows MAESTRO UI with CODE EXECUTOR showing partial output, but PLANNER panel is **completely empty** despite agent execution.

**Root Cause Identified**: **12 out of 15 agents** (80%) are missing `execute_streaming()` method required for real-time UI updates.

**Impact**:
- ‚ùå No real-time streaming for PLANNER, EXPLORER, REVIEWER, etc.
- ‚ùå Users see empty panels during execution
- ‚ùå 30 FPS streaming architecture present but unused
- ‚ùå UI shows "Thinking..." but no actual LLM output

---

## SCREENSHOT ANALYSIS

### Observed Behavior (from provided screenshot)

**Top Section (Before Approval)**:
```
‚ö†Ô∏è  APPROVAL REQUIRED
echo "1. Ferva √°gua.\n2. Coloque o milho na √°gua fervente por 3
minutos.\n3. Adicione o tempero.\n4. Misture bem e sirva."
```
‚úÖ Approval system working

**Bottom Section (During Execution)**:

| Panel | Status | Content Observed |
|-------|--------|------------------|
| **CODE EXECUTOR ‚ö°** | üü° PARTIAL | Shows "ü§î Thinking..." + partial echo output |
| **PLANNER üéØ** | ‚ùå EMPTY | Completely blank (should show planning) |
| **FILE OPERATIONS üìÅ** | ‚ùå EMPTY | "No file operations yet" |

### Expected Behavior

| Panel | Should Show |
|-------|-------------|
| **CODE EXECUTOR** | Token-by-token streaming of command generation |
| **PLANNER** | Real-time plan steps as LLM generates them |
| **FILE OPERATIONS** | Live updates when files are read/written |

---

## ARCHITECTURAL ANALYSIS

### Current Architecture (What EXISTS)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     MAESTRO UI v10.0                        ‚îÇ
‚îÇ                  (maestro_shell_ui.py)                      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ CODE EXECUTOR‚îÇ  ‚îÇ   PLANNER    ‚îÇ  ‚îÇ FILE OPS     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ update_agent_‚îÇ  ‚îÇ update_agent_‚îÇ  ‚îÇ add_file_    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ _stream()    ‚îÇ  ‚îÇ _stream()    ‚îÇ  ‚îÇ operation()  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                          ‚ñ≤                                  ‚îÇ
‚îÇ                          ‚îÇ                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚úÖ UI Layer: READY
                           ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Orchestrator                            ‚îÇ
‚îÇ               (maestro_v10_integrated.py)                   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   async def execute_streaming(prompt, context):            ‚îÇ
‚îÇ       agent_name = self.route(prompt)                      ‚îÇ
‚îÇ       agent = self.agents[agent_name]                      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ       if hasattr(agent, 'execute_streaming'):              ‚îÇ
‚îÇ           async for update in agent.execute_streaming():   ‚îÇ
‚îÇ               yield update  ‚Üê‚îÄ STREAMS TO UI               ‚îÇ
‚îÇ       else:                                                 ‚îÇ
‚îÇ           result = await agent.execute()  ‚Üê FALLBACK       ‚îÇ
‚îÇ           yield {"type": "result", "data": result}         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚úÖ Orchestrator: READY
                           ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Agent Layer                            ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ  ‚îÇ NextGenExecutor  ‚îÇ  ‚îÇ  PlannerAgent    ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ ‚úÖ execute_     ‚îÇ  ‚îÇ ‚ùå NO execute_  ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ    streaming()   ‚îÇ  ‚îÇ    streaming()   ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ Yields:          ‚îÇ  ‚îÇ Only has:        ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ - thinking       ‚îÇ  ‚îÇ - execute()      ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ - command        ‚îÇ  ‚îÇ   (returns final)‚îÇ               ‚îÇ
‚îÇ  ‚îÇ - executing      ‚îÇ  ‚îÇ                  ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ - result         ‚îÇ  ‚îÇ                  ‚îÇ               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ         ‚úÖ                      ‚ùå                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### THE GAP

**The MISSING piece**: 12 agents don't implement `execute_streaming()`, so Orchestrator falls back to non-streaming execution, causing empty panels.

---

## DETAILED AGENT AUDIT

### ‚úÖ Agents WITH Streaming (3/15 - 20%)

| Agent | File | Status | Streaming Quality |
|-------|------|--------|-------------------|
| **NextGenExecutor** | `executor_nextgen.py` | ‚úÖ COMPLETE | Excellent (30 FPS ready) |
| **Executor (legacy)** | `executor.py` | ‚úÖ PARTIAL | Basic streaming |
| **DataAgent** | `data_agent_production.py` | ‚úÖ COMPLETE | Production-ready |

**NextGenExecutor Streaming Implementation** (Reference):
```python
async def execute_streaming(self, task: AgentTask) -> AsyncIterator[Dict[str, Any]]:
    """
    Streaming execution with real-time updates (30 FPS)
    Yields: {"type": "thinking"|"command"|"executing"|"result", "data": ...}
    """
    # 1. Stream thinking phase
    yield {"type": "status", "data": "ü§î Thinking..."}

    command_buffer = []
    async for token in self._stream_command_generation(task.request, task.context):
        command_buffer.append(token)
        yield {"type": "thinking", "data": token}  # ‚Üê Real-time tokens!

    command = ''.join(command_buffer).strip()
    yield {"type": "command", "data": command}

    # 2. Security validation
    yield {"type": "status", "data": "üîí Validating..."}

    # 3. Execute command
    yield {"type": "executing", "data": "Running..."}
    result = await self._execute_command(command)

    # 4. Final result
    yield {"type": "result", "data": result}
```

### ‚ùå Agents WITHOUT Streaming (12/15 - 80%)

| Priority | Agent | File | Impact | Current Behavior |
|----------|-------|------|--------|------------------|
| üî¥ P0 | **PlannerAgent** | `planner.py` | CRITICAL | Empty panel in UI |
| üî¥ P0 | **ExplorerAgent** | `explorer.py` | CRITICAL | Empty panel in UI |
| üü† P1 | **ReviewerAgent** | `reviewer.py` | HIGH | No live review feedback |
| üü† P1 | **RefactorerAgent** | `refactorer.py` | HIGH | No live refactor progress |
| üü° P2 | **ArchitectAgent** | `architect.py` | MEDIUM | No feasibility analysis stream |
| üü° P2 | **SecurityAgent** | `security.py` | MEDIUM | No live security scan |
| üü° P2 | **PerformanceAgent** | `performance.py` | MEDIUM | No live benchmark stream |
| üü° P2 | **TestingAgent** | `testing.py` | MEDIUM | No live test execution |
| üü¢ P3 | **DocumentationAgent** | `documentation.py` | LOW | Docs generation not time-critical |
| üü¢ P3 | **DevOpsAgent** | `devops_agent.py` | LOW | CI/CD not time-critical |
| üü¢ P3 | **RefactorerV8** | `refactorer_v8.py` | LOW | Duplicate/legacy |
| üü¢ P3 | **LLMAdapter** | `llm_adapter.py` | LOW | Not a user-facing agent |

---

## IMPACT ANALYSIS

### User Experience Impact

**Current State** (as shown in screenshot):
1. User types: "me da uma receita de miojo"
2. Maestro routes to CODE EXECUTOR
3. CODE EXECUTOR streams tokens ‚úÖ (shows "Thinking...")
4. User needs context from PLANNER
5. PLANNER panel is **completely empty** ‚ùå
6. User doesn't know what plan is being created
7. FILE OPERATIONS shows "No file operations yet" ‚ùå

**Result**: User sees 2/3 panels empty despite agent working.

### Performance Impact

- ‚ùå **30 FPS streaming architecture UNUSED** for 80% of agents
- ‚ùå **UI refresh working** but no data to display
- ‚ùå **Network bandwidth wasted** (full responses transmitted at once vs. streaming)
- ‚ùå **Perceived latency HIGH** (no intermediate feedback)

### Developer Impact

- ‚ùå **Inconsistent agent interfaces** (some stream, some don't)
- ‚ùå **UI code has dead pathways** (panels ready but agents don't feed them)
- ‚ùå **Debugging difficult** (can't see agent reasoning in real-time)

---

## ROOT CAUSE ANALYSIS

### Why This Happened

**Historical Context** (from codebase archaeology):

1. **Phase 1**: BaseAgent created with only `execute()` method
2. **Phase 2**: NextGenExecutor added `execute_streaming()` for performance
3. **Phase 3**: MAESTRO UI v10.0 built expecting all agents to stream
4. **Phase 4**: **GAP CREATED** - UI assumes streaming, but agents don't provide it

### The Mismatch

```python
# maestro_v10_integrated.py (Orchestrator)
async for update in self.orch.execute_streaming(q, context={'cwd': ...}):
    if update["type"] == "thinking":
        await self.maestro_ui.update_agent_stream(agent_name, token)
        #      ‚ñ≤                                   ‚ñ≤
        #      ‚îÇ                                   ‚îî‚îÄ UI READY to display
        #      ‚îî‚îÄ But most agents DON'T yield "thinking" updates!
```

```python
# planner.py (PlannerAgent)
async def execute(self, task: AgentTask) -> AgentResponse:
    # Thinks internally, NO streaming
    plan = await self._generate_plan(task)

    # Returns FINAL result only
    return AgentResponse(success=True, data={"plan": plan})
    # ‚ùå NO intermediate updates yielded
```

**Result**: Orchestrator calls `agent.execute()` (fallback), gets final result, UI panels stay empty until completion.

---

## SOLUTION ARCHITECTURE

### Overview

To fix streaming, we need to add `execute_streaming()` to all critical agents following the pattern established by NextGenExecutor.

### Streaming Contract (Interface)

```python
async def execute_streaming(
    self,
    task: AgentTask
) -> AsyncIterator[Dict[str, Any]]:
    """
    Stream agent execution with real-time updates.

    Yields dictionaries with structure:
        {
            "type": "thinking" | "status" | "command" | "executing" | "result",
            "data": <content>,
            "meta": {Optional metadata}
        }

    Update Types:
        - "thinking": LLM token-by-token generation
        - "status": Status messages (e.g., "Validating...", "Loading context...")
        - "command": Generated command/action
        - "executing": Execution in progress
        - "result": Final result (required, terminal event)

    Example Flow:
        yield {"type": "status", "data": "Loading files..."}
        yield {"type": "thinking", "data": "Based on"}
        yield {"type": "thinking", "data": " the code"}
        yield {"type": "command", "data": "refactor_function(...)"}
        yield {"type": "result", "data": AgentResponse(...)}
    """
```

### Implementation Pattern (Template)

```python
async def execute_streaming(
    self,
    task: AgentTask
) -> AsyncIterator[Dict[str, Any]]:
    """Streaming execution for [AgentName]"""

    # Phase 1: Context gathering (with status updates)
    yield {"type": "status", "data": "üîç Gathering context..."}
    context = await self._gather_context(task)

    # Phase 2: LLM generation (with token streaming)
    yield {"type": "status", "data": "ü§î Analyzing..."}

    response_buffer = []
    async for token in self.llm.generate_stream(prompt, context):
        response_buffer.append(token)
        yield {"type": "thinking", "data": token}  # ‚Üê KEY: Stream tokens!

    response_text = ''.join(response_buffer)

    # Phase 3: Processing (with status updates)
    yield {"type": "status", "data": "‚öôÔ∏è  Processing..."}
    processed_data = await self._process_response(response_text, task)

    # Phase 4: Tool execution (if needed, with updates)
    if requires_tools:
        yield {"type": "status", "data": "üîß Executing tools..."}
        tool_results = await self._execute_tools(processed_data)

    # Phase 5: Final result (required)
    final_result = AgentResponse(
        success=True,
        data=processed_data,
        reasoning=response_text
    )

    yield {"type": "result", "data": final_result}
```

### LLM Streaming Helper

**Problem**: Current LLMClient may not support streaming.

**Solution**: Add streaming method to LLMClient

```python
# qwen_dev_cli/core/llm.py

async def generate_stream(
    self,
    prompt: str,
    context: Optional[Dict] = None,
    **kwargs
) -> AsyncIterator[str]:
    """
    Stream LLM generation token-by-token.

    Yields individual tokens as they're generated.
    """
    if self.provider == "gemini":
        # Gemini streaming API
        response = await self.client.generate_content_async(
            prompt,
            stream=True,  # ‚Üê Enable streaming
            **kwargs
        )

        async for chunk in response:
            if chunk.text:
                yield chunk.text

    elif self.provider == "ollama":
        # Ollama streaming
        async for chunk in self.client.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            stream=True
        ):
            yield chunk['message']['content']

    else:
        # Fallback: Simulate streaming by splitting response
        full_response = await self.generate(prompt, context, **kwargs)
        for token in full_response.split():
            yield token + " "
            await asyncio.sleep(0.01)  # Simulate streaming delay
```

---

## IMPLEMENTATION PLAN

### Phase 1: Critical Agents (P0) - Required for Basic Functionality

**Priority**: üî¥ CRITICAL
**Agents**: PlannerAgent, ExplorerAgent
**Timeline**: Implement first

#### 1.1 PlannerAgent Streaming

**File**: `qwen_dev_cli/agents/planner.py`

**Changes Required**:

```python
# ADD THIS METHOD

async def execute_streaming(
    self,
    task: AgentTask
) -> AsyncIterator[Dict[str, Any]]:
    """Stream plan generation with real-time updates"""

    # 1. Load context
    yield {"type": "status", "data": "üìã Loading project context..."}
    context = await self._gather_context(task)

    # 2. Stream plan generation
    yield {"type": "status", "data": "üéØ Generating plan..."}

    prompt = self._build_prompt(task, context)

    plan_buffer = []
    async for token in self.llm.generate_stream(prompt):
        plan_buffer.append(token)
        yield {"type": "thinking", "data": token}  # ‚Üê Stream to PLANNER panel

    plan_text = ''.join(plan_buffer)

    # 3. Parse plan into steps
    yield {"type": "status", "data": "‚öôÔ∏è  Parsing plan steps..."}
    parsed_plan = self._parse_plan(plan_text)

    # 4. Validate plan
    yield {"type": "status", "data": "‚úÖ Validating plan..."}
    validated_plan = await self._validate_plan(parsed_plan, context)

    # 5. Final result
    result = AgentResponse(
        success=True,
        data={"plan": validated_plan, "raw_plan": plan_text},
        reasoning=f"Generated {len(validated_plan)} steps"
    )

    yield {"type": "result", "data": result}
```

**Dependencies**:
- Requires `LLMClient.generate_stream()` method
- No other breaking changes

**Testing**:
```python
# Test streaming works
planner = PlannerAgent(llm, mcp)
task = AgentTask(request="Create a login feature")

async for update in planner.execute_streaming(task):
    print(f"[{update['type']}] {update.get('data', '')[:50]}")
    # Should print:
    # [status] üìã Loading project context...
    # [status] üéØ Generating plan...
    # [thinking] Based on
    # [thinking]  the request
    # [thinking] , I will
    # ...
    # [result] AgentResponse(...)
```

#### 1.2 ExplorerAgent Streaming

**File**: `qwen_dev_cli/agents/explorer.py`

**Changes Required**:

```python
async def execute_streaming(
    self,
    task: AgentTask
) -> AsyncIterator[Dict[str, Any]]:
    """Stream code exploration with real-time updates"""

    # 1. Scan filesystem
    yield {"type": "status", "data": "üó∫Ô∏è  Scanning codebase..."}
    file_tree = await self._scan_directory(task.context.get('cwd', '.'))
    yield {"type": "status", "data": f"Found {len(file_tree)} files"}

    # 2. Build context
    yield {"type": "status", "data": "üìñ Building context..."}
    context = await self._build_context(task, file_tree)

    # 3. Stream analysis
    yield {"type": "status", "data": "üîç Analyzing code structure..."}

    prompt = self._build_exploration_prompt(task, context)

    analysis_buffer = []
    async for token in self.llm.generate_stream(prompt):
        analysis_buffer.append(token)
        yield {"type": "thinking", "data": token}  # ‚Üê Stream to EXPLORER panel

    analysis = ''.join(analysis_buffer)

    # 4. Extract findings
    yield {"type": "status", "data": "üìä Extracting insights..."}
    findings = self._extract_findings(analysis)

    # 5. Final result
    result = AgentResponse(
        success=True,
        data={"findings": findings, "file_tree": file_tree},
        reasoning=analysis
    )

    yield {"type": "result", "data": result}
```

### Phase 2: High-Impact Agents (P1)

**Priority**: üü† HIGH
**Agents**: ReviewerAgent, RefactorerAgent
**Timeline**: Implement after P0

Same pattern as above, adapted for each agent's specific workflow.

### Phase 3: Medium-Impact Agents (P2)

**Priority**: üü° MEDIUM
**Agents**: ArchitectAgent, SecurityAgent, PerformanceAgent, TestingAgent
**Timeline**: Implement incrementally

### Phase 4: Low-Priority Agents (P3)

**Priority**: üü¢ LOW
**Agents**: DocumentationAgent, DevOpsAgent
**Timeline**: Optional, nice-to-have

---

## FILE OPERATIONS STREAMING

**Separate Issue**: FILE OPERATIONS panel shows "No file operations yet"

### Root Cause

**File Tracker Integration Missing**

```python
# maestro_v10_integrated.py (line 817)
self.file_tracker = FileOperationTracker()
self.file_tracker.set_callback(self.maestro_ui.add_file_operation)
```

**Problem**: Agents don't call `file_tracker.track_operation()` when reading/writing files.

### Solution

**Option A: Automatic Tracking in Tool Execution**

Wrap MCP tool execution to automatically track file operations:

```python
# qwen_dev_cli/core/mcp.py

async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
    """Execute tool with automatic file operation tracking"""

    # Execute tool
    result = await tool._execute_validated(**arguments)

    # Track file operations
    if tool_name in ['read_file', 'write_file', 'edit_file', 'move_file', 'copy_file']:
        self._track_file_operation(tool_name, arguments, result)

    return result

def _track_file_operation(self, tool_name, arguments, result):
    """Track file operation for UI display"""
    if hasattr(self, 'file_tracker') and self.file_tracker:
        operation = {
            'type': tool_name,
            'path': arguments.get('path') or arguments.get('file_path'),
            'success': result.success if hasattr(result, 'success') else True,
            'timestamp': datetime.now()
        }
        self.file_tracker.track_operation(operation)
```

**Option B: Explicit Tracking in Agents**

Each agent manually tracks file operations:

```python
# In agent's execute_streaming()
async for update in agent.execute_streaming(task):
    if update["type"] == "file_operation":
        self.file_tracker.track_operation(update["data"])

    yield update
```

**Recommendation**: Use Option A (automatic) for consistency and less agent code duplication.

---

## TESTING STRATEGY

### Unit Tests

```python
# tests/unit/test_planner_streaming.py

@pytest.mark.asyncio
async def test_planner_streams_thinking_tokens():
    """PlannerAgent should yield thinking tokens during execution"""
    planner = PlannerAgent(mock_llm, mock_mcp)
    task = AgentTask(request="Create feature X")

    thinking_tokens = []
    async for update in planner.execute_streaming(task):
        if update["type"] == "thinking":
            thinking_tokens.append(update["data"])

    # Should have received multiple tokens
    assert len(thinking_tokens) > 10

    # Tokens should form coherent text
    full_text = ''.join(thinking_tokens)
    assert len(full_text) > 100

@pytest.mark.asyncio
async def test_planner_yields_final_result():
    """PlannerAgent streaming must yield final result"""
    planner = PlannerAgent(mock_llm, mock_mcp)
    task = AgentTask(request="Create feature X")

    final_result = None
    async for update in planner.execute_streaming(task):
        if update["type"] == "result":
            final_result = update["data"]

    assert final_result is not None
    assert isinstance(final_result, AgentResponse)
    assert final_result.success
```

### Integration Tests

```python
# tests/integration/test_maestro_streaming.py

@pytest.mark.asyncio
async def test_maestro_ui_displays_planner_stream():
    """MAESTRO UI should display PlannerAgent streaming"""
    from maestro_v10_integrated import Shell

    shell = Shell()
    shell.init()

    # Capture UI updates
    ui_updates = []
    original_update = shell.maestro_ui.update_agent_stream

    async def capture_update(agent_name, text, *args, **kwargs):
        ui_updates.append((agent_name, text))
        await original_update(agent_name, text, *args, **kwargs)

    shell.maestro_ui.update_agent_stream = capture_update

    # Execute command that routes to PlannerAgent
    await shell.orch.execute_streaming(
        "Plan a refactoring for auth module",
        context={'cwd': '.'}
    )

    # Verify UI received updates
    assert len(ui_updates) > 0

    # Verify planner panel got updates
    planner_updates = [u for u in ui_updates if u[0] == 'planner']
    assert len(planner_updates) > 10  # Should have multiple tokens
```

### Manual Testing Checklist

```
‚ñ° Launch MAESTRO: ./maestro
‚ñ° Enter command: "Create a plan for implementing user auth"
‚ñ° Observe PLANNER panel during execution:
  ‚ñ° Shows "üìã Loading project context..." status
  ‚ñ° Shows "üéØ Generating plan..." status
  ‚ñ° Shows LLM tokens streaming in real-time
  ‚ñ° Text flows smoothly (30 FPS)
  ‚ñ° Final plan appears formatted

‚ñ° Enter command: "Analyze the codebase structure"
‚ñ° Observe EXPLORER panel during execution:
  ‚ñ° Shows "üó∫Ô∏è  Scanning codebase..." status
  ‚ñ° Shows file count status
  ‚ñ° Shows LLM analysis streaming
  ‚ñ° Final findings appear

‚ñ° Enter command: "Write a simple hello world script"
‚ñ° Observe FILE OPERATIONS panel:
  ‚ñ° Shows file write operation appear
  ‚ñ° Shows file path and timestamp
  ‚ñ° Status icon updates (success/failure)
```

---

## ROLLOUT STRATEGY

### Phase 1: Foundation (Week 1)

**Goal**: Enable streaming infrastructure

**Tasks**:
1. ‚úÖ Add `LLMClient.generate_stream()` method
2. ‚úÖ Test streaming with NextGenExecutor (already works)
3. ‚úÖ Verify MAESTRO UI receives streams correctly
4. ‚úÖ Add automatic file operation tracking to MCP

**Success Criteria**:
- NextGenExecutor streams visible in UI
- File operations tracked automatically
- No regressions

### Phase 2: Critical Agents (Week 2)

**Goal**: Fix P0 agents (PlannerAgent, ExplorerAgent)

**Tasks**:
1. ‚úÖ Implement `PlannerAgent.execute_streaming()`
2. ‚úÖ Implement `ExplorerAgent.execute_streaming()`
3. ‚úÖ Write unit tests for both
4. ‚úÖ Test in MAESTRO UI
5. ‚úÖ Fix any issues found

**Success Criteria**:
- PLANNER panel shows real-time plan generation
- EXPLORER panel shows real-time analysis
- No empty panels during execution
- 30 FPS smooth streaming

### Phase 3: High-Impact Agents (Week 3-4)

**Goal**: Add streaming to ReviewerAgent, RefactorerAgent

**Tasks**:
1. Implement ReviewerAgent.execute_streaming()
2. Implement RefactorerAgent.execute_streaming()
3. Test and validate

**Success Criteria**:
- All P0 + P1 agents stream to UI
- User experience significantly improved

### Phase 4: Remaining Agents (Ongoing)

**Goal**: Add streaming to P2 and P3 agents incrementally

**Tasks**: Implement remaining agents as needed

---

## SUCCESS METRICS

### Technical Metrics

| Metric | Target | How to Measure |
|--------|--------|----------------|
| Agents with streaming | 100% (15/15) | Audit script |
| UI panel usage | 100% | All panels show content |
| Streaming latency | < 50ms | Time from LLM token to UI |
| FPS during streaming | ‚â• 25 FPS | PerformanceMonitor |
| Token throughput | > 50 tokens/sec | Measure in UI |

### User Experience Metrics

| Metric | Target | How to Measure |
|--------|--------|----------------|
| Empty panel occurrences | 0 | Visual inspection |
| User can see progress | 100% of time | Observation |
| Perceived latency | Low | User feedback |

---

## KNOWN ISSUES & WORKAROUNDS

### Issue #1: LLMClient doesn't have generate_stream()

**Workaround**: Implement it (see Solution Architecture section)

### Issue #2: Some agents have complex multi-step workflows

**Workaround**: Stream each phase separately:
```python
# Phase 1
yield {"type": "status", "data": "Phase 1: Loading..."}
async for token in phase1_stream():
    yield {"type": "thinking", "data": token}

# Phase 2
yield {"type": "status", "data": "Phase 2: Analyzing..."}
async for token in phase2_stream():
    yield {"type": "thinking", "data": token}
```

### Issue #3: Streaming increases LLM API costs (slightly)

**Impact**: Minimal (streaming uses same tokens)
**Mitigation**: Streaming is more efficient (can cancel early)

---

## REFERENCES

### Code Files to Modify

| Priority | File | Changes Required |
|----------|------|------------------|
| P0 | `qwen_dev_cli/core/llm.py` | Add `generate_stream()` method |
| P0 | `qwen_dev_cli/agents/planner.py` | Add `execute_streaming()` |
| P0 | `qwen_dev_cli/agents/explorer.py` | Add `execute_streaming()` |
| P0 | `qwen_dev_cli/core/mcp.py` | Add file operation tracking |
| P1 | `qwen_dev_cli/agents/reviewer.py` | Add `execute_streaming()` |
| P1 | `qwen_dev_cli/agents/refactorer.py` | Add `execute_streaming()` |
| P2 | Other agents | Add `execute_streaming()` |

### Files Already Correct (No Changes)

- ‚úÖ `maestro_v10_integrated.py` (Orchestrator handles streaming correctly)
- ‚úÖ `qwen_dev_cli/tui/components/maestro_shell_ui.py` (UI ready for streaming)
- ‚úÖ `qwen_dev_cli/agents/executor_nextgen.py` (Reference implementation)

### Architecture Diagram Files

```
docs/architecture/
‚îú‚îÄ‚îÄ streaming_flow.png (create: shows token flow)
‚îú‚îÄ‚îÄ agent_interface.md (create: documents execute_streaming contract)
‚îî‚îÄ‚îÄ maestro_ui_panels.md (create: documents panel update protocol)
```

---

## APPENDIX A: Streaming Update Types

| Type | Purpose | Data Format | Example |
|------|---------|-------------|---------|
| `thinking` | LLM token-by-token | String (single token or word) | "Based" |
| `status` | Status message | String (full message) | "üîç Loading files..." |
| `command` | Generated command | String (command text) | "echo 'hello'" |
| `executing` | Tool execution | String (status message) | "Running command..." |
| `result` | Final result | AgentResponse or Dict | `AgentResponse(...)` |

---

## APPENDIX B: Quick Reference - Implement Streaming

**Copy-paste template for adding streaming to any agent**:

```python
async def execute_streaming(
    self,
    task: AgentTask
) -> AsyncIterator[Dict[str, Any]]:
    """Stream execution for [YOUR AGENT NAME]"""

    # 1. Pre-processing
    yield {"type": "status", "data": "üîÑ Starting [agent name]..."}

    # 2. Main LLM generation (STREAMING!)
    yield {"type": "status", "data": "ü§î Analyzing..."}

    prompt = self._build_prompt(task)
    response_buffer = []

    async for token in self.llm.generate_stream(prompt):
        response_buffer.append(token)
        yield {"type": "thinking", "data": token}  # ‚Üê CRITICAL: Stream tokens!

    response_text = ''.join(response_buffer)

    # 3. Post-processing
    yield {"type": "status", "data": "‚öôÔ∏è  Processing..."}
    processed = self._process(response_text)

    # 4. Final result (REQUIRED!)
    result = AgentResponse(
        success=True,
        data=processed,
        reasoning=response_text
    )

    yield {"type": "result", "data": result}
```

---

**Report Version**: 1.0
**Last Updated**: 2025-11-24
**Next Review**: After Phase 1 implementation

**For Implementer**: This report contains EVERYTHING needed to fix streaming. Start with Phase 1 (LLMClient), then Phase 2 (PlannerAgent + ExplorerAgent). The architecture is sound, we just need to implement the streaming methods.

üéØ **Priority**: CRITICAL - This is core UX functionality
