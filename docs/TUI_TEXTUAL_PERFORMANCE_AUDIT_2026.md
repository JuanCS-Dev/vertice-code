# üî¨ Textual TUI Performance Audit (Vertice TUI) ‚Äî 2026
**Repo:** Vertice-Code (`src/vertice_tui/`)
**Date:** 2026-01-19
**Scope:** performance + responsiveness + long-session stability (render / reactivity / workers / lifecycle / memory).
**Input doc (Phase 1):** `PHASE_1_REPORT.md` (baseline Textual 6.2.1, pytest-asyncio 0.24.0).

> Este documento √© um relat√≥rio de auditoria + backlog priorizado. Ele **n√£o** implementa mudan√ßas (PRs separadas), mas define o caminho mais cient√≠fico para otimizar.

---

## 0) Padr√µes oficiais (Textual) usados como refer√™ncia

Consolidado em `PHASE_1_REPORT.md`:
- App lifecycle / mounting: https://textual.textualize.io/guide/app/
- Workers (usar para tarefas longas / evitar bloquear a UI): https://textual.textualize.io/guide/workers/ + https://textual.textualize.io/api/work/
- Reactivity (watchers leves; evitar ‚Äúrefresh storm‚Äù): https://textual.textualize.io/guide/reactivity/
- Widgets / growth control (evitar crescimento infinito): https://textual.textualize.io/guide/widgets/
- Lazy instantiation (montagem tardia de widgets caros): https://textual.textualize.io/api/lazy/

Regra-guia (derivada dos guias acima):
1) **Nada bloqueante** (disk/network/CPU pesado) no **UI thread** nem em ‚Äúworkers async‚Äù (que rodam no mesmo event loop).
2) UI updates devem ser **batched** e **throttled** em caminhos de alta frequ√™ncia (typing + streaming).

---

## 1) Executive Summary (P0/P1)

### P0 ‚Äî maiores ganhos, menor risco
1) **Autocomplete**: hoje calcula completions e reconstr√≥i widgets a cada tecla (UI thread) ‚Üí input lag prov√°vel.
2) **Streaming markdown**: `ResponseView.append_chunk()` concatena string por delta (`+=`) e escreve no MarkdownStream por delta ‚Üí risco de **O(n¬≤)** + ‚Äúupdate storm‚Äù com modelos r√°pidos.
3) **Comandos com I/O**: `/read`, `/add`, `/image`, `/pdf` etc fazem `Path.read_text()` / scanning / parsing s√≠ncronos dentro de handlers async ‚Üí UI pode congelar apesar de `run_worker`.

### P1 ‚Äî ganhos relevantes / mudan√ßas moderadas
4) **Bridge init no on_mount**: `VerticeApp.on_mount()` acessa `self.bridge` (singleton pesado) antes de UI estar est√°vel ‚Üí tempo de startup e ‚Äúfirst interaction delay‚Äù.
5) **Evitar exce√ß√µes em watchers**: `StatusBar.watch_token_*` tenta atualizar `#tokens` (id n√£o existe) ‚Üí exce√ß√£o/try/except em hot path se tokens forem atualizados frequentemente.

---

## 2) Observa√ß√µes objetivas (o que j√° est√° bom)

### 2.1 Render & scrollback
- `ResponseView` implementa **limite de scrollback** (`VERTICE_TUI_MAX_VIEW_ITEMS`, default 300) via `_trim_view_items()` ‚Üí controla crescimento em sess√µes longas. (`src/vertice_tui/widgets/response_view.py`)
- Scroll throttling no stream: `scroll_end` no m√°ximo 20fps (50ms) ‚Üí reduz thrash de layout. (`src/vertice_tui/widgets/response_view.py`)

### 2.2 Streaming SSE resiliente
- Parsing incremental via `OpenResponsesParser.feed(line)` e fallback para linhas n√£o‚ÄëSSE (evita sumir texto). (`src/vertice_tui/app.py`, `src/vertice_tui/core/openresponses_events.py`)

### 2.3 Workers j√° usados no caminho principal
- Chat/commands s√£o disparados via `run_worker(..., group="chat_dispatch", exclusive=True)` para n√£o travar handler de input. (`src/vertice_tui/app.py`)

---

## 3) Achados e recomenda√ß√µes (por componente)

### 3.1 `VerticeApp` (input + lifecycle)
**Arquivo:** `src/vertice_tui/app.py`

**Achado A ‚Äî Autocomplete no UI thread**
- `on_input_changed()` chama `self.bridge.autocomplete.get_completions(...)` diretamente e atualiza o dropdown a cada tecla.
- `get_completions()` pode fazer scanning (modo `@`) e fuzzy scoring; mesmo com limites/caches, o pior caso √© vis√≠vel em repos grandes.

**Recomenda√ß√£o A (P0) ‚Äî Debounce + offload + stale-guard**
- Debounce (ex.: 75‚Äì150ms) para typing r√°pido.
- Offload da computa√ß√£o de completions (principalmente `@`) com `await asyncio.to_thread(...)`.
- Stale-guard: aplicar resultado **somente** se o input atual ainda for o mesmo (evita ‚Äúautocomplete atrasado‚Äù).
- Use `run_worker(..., group="autocomplete", exclusive=True)` para cancelamento determin√≠stico (padr√£o Workers).

**Achado B ‚Äî Bridge init pode bloquear**
- `on_mount()` usa `self.bridge` v√°rias vezes (status, dashboard, mensagens iniciais) antes de disparar `bridge.warmup()` (que √© background).

**Recomenda√ß√£o B (P1) ‚Äî Startup em 2 fases**
- Fase 1 (r√°pida, s√≠ncrona): montar UI + banner + focus prompt.
- Fase 2 (worker): `bridge = await asyncio.to_thread(get_bridge)`; depois atualizar StatusBar/TokenDashboard e disparar warmup.
- UI deve mostrar estado ‚ÄúLoading bridge‚Ä¶‚Äù at√© fase 2 concluir.

---

### 3.2 `ResponseView` (streaming / render)
**Arquivo:** `src/vertice_tui/widgets/response_view.py`

**Achado A ‚Äî concatena√ß√£o O(n¬≤)**
- `append_chunk()` faz `self.current_response += chunk` por delta, mesmo quando usa `TextualMarkdown.get_stream()` (onde o stream j√° mant√©m o estado do render).

**Recomenda√ß√£o A (P0) ‚Äî Buffer + flush em frequ√™ncia fixa**
- Trocar `current_response` por **buffer de chunks** (lista/deque) durante streaming.
- ‚ÄúFlush‚Äù no m√°ximo 20‚Äì30fps (ex.: a cada 33‚Äì50ms):
  - juntar chunks (`"".join(buffer)`) e fazer **uma** escrita no MarkdownStream.
  - manter `current_response` apenas no final (via `response.output_text.done`) ou quando necess√°rio (copy/export).
- Resultado esperado: menos repaints, menos GC, throughput mais est√°vel.

**Achado B ‚Äî `end_thinking()` chamado redundante**
- `ResponseView.handle_open_responses_event()` chama `await end_thinking()` em `response.completed`.
- `VerticeApp._handle_chat()` chama `await view.end_thinking()` no `finally`.

**Recomenda√ß√£o B (P2) ‚Äî single ‚Äúfinalizer‚Äù**
- Definir um √∫nico ponto de finaliza√ß√£o (UI) para reduzir trabalho duplicado e simplificar invariantes.

---

### 3.3 Autocomplete UI (dropdown)
**Arquivo:** `src/vertice_tui/widgets/autocomplete.py`

**Achado ‚Äî remount em loop**
- `show_completions()` remove e remonta at√© 15 widgets a cada update.

**Recomenda√ß√£o (P0) ‚Äî reciclar widgets**
- Pr√©-criar N itens (15) no `on_mount`/`compose`, e somente:
  - `update(text)`
  - alternar classes (`selected`, `item-tool`, etc)
  - mostrar/esconder itens n√£o usados
- Evita churn de mount/layout e reduz custo por keystroke.

---

### 3.4 Autocomplete ‚Äúengine‚Äù (file scan / fuzzy)
**Arquivo:** `src/vertice_tui/core/ui_bridge.py` (`AutocompleteBridge`)

**Achado ‚Äî scanning de arquivos √© s√≠ncrono**
- `_scan_files()` percorre diret√≥rios e faz `iterdir()`/`is_dir()`/`is_file()` no caminho cr√≠tico do autocomplete `@`.
- Mesmo com `MAX_FILES=300` e `MAX_DEPTH=4`, isso pode travar em FS lento / repo grande.

**Recomenda√ß√£o (P0)**
- Pre-warm do cache em background (`bridge.warmup()` ou worker dedicado).
- Offload do primeiro scan com `asyncio.to_thread(self._scan_files)`.
- Cache incremental por query (√∫ltimo `@query` ‚Üí resultados) para evitar sort/scoring completo a cada tecla.

---

### 3.5 Command handlers (I/O e opera√ß√µes pesadas)
**Arquivos principais:**
- `src/vertice_tui/app.py` (`_read_file`)
- `src/vertice_tui/handlers/basic.py` (`/read`)
- `src/vertice_tui/handlers/context_commands.py` (`/add`)
- `src/vertice_tui/core/context/file_window.py` (`FileContextEntry.from_file`)
- `src/vertice_tui/handlers/operations.py` (`/image`, `/pdf`, etc)

**Achado ‚Äî ‚Äúasync‚Äù com I/O s√≠ncrono ainda bloqueia**
- Muitos handlers s√£o `async def`, mas fazem `Path.read_text()` e outras opera√ß√µes s√≠ncronas (disk).
- Como os ‚Äúworkers‚Äù do Textual s√£o async por padr√£o, isso **continua** bloqueando o event loop quando executado dentro deles.

**Recomenda√ß√£o (P0)**
- Padronizar: qualquer disk I/O / CPU pesado em handlers ‚Üí `await asyncio.to_thread(...)`.
- Para opera√ß√µes muito pesadas, usar `run_worker(..., group="ops", exclusive=False)` + progress UI.

---

### 3.6 StatusBar (watchers / hot path)
**Arquivo:** `src/vertice_tui/widgets/status_bar.py`

**Achado ‚Äî exce√ß√µes em watchers**
- `watch_token_used()` / `watch_token_limit()` chamam `_update_element("#tokens", ...)`, mas o widget `#tokens` n√£o existe no `compose()`. Isso gera exce√ß√£o e logging (mesmo que debug), o que √© custo desnecess√°rio.

**Recomenda√ß√£o (P1)**
- Ajustar watchers para atualizar apenas elementos existentes (ou adicionar o elemento faltante).
- Evitar `try/except` em caminho quente onde updates podem ser frequentes.

---

### 3.7 InputArea (TextArea / language detection)
**Arquivo:** `src/vertice_tui/widgets/input_area.py`

**Achado ‚Äî custo por tecla cresce com texto**
- `_detect_language()` faz m√∫ltiplos `in` sobre o texto completo.
- `_update_status()` faz `len(text.split("\n"))` a cada mudan√ßa.

**Recomenda√ß√£o (P2)**
- Limitar detec√ß√£o a um ‚Äúwindow‚Äù (ex.: primeiros/√∫ltimos 2‚Äì4k chars) em vez do buffer inteiro.
- Contar linhas com `text.count("\\n") + 1` para reduzir aloca√ß√µes.
- Debounce de status/language para typing r√°pido (mesmo timer do autocomplete, se houver).

---

## 4) Backlog priorizado (mudan√ßas pequenas, mensur√°veis)

| ID | Prioridade | √Årea | Mudan√ßa proposta | M√©trica-alvo | Valida√ß√£o |
|---:|:--:|---|---|---|---|
| 1 | P0 | Autocomplete | Debounce + `to_thread` + stale-guard + worker `group="autocomplete"` | p95 keystroke‚ÜíUI < 30ms | benchmark typing + log timings |
| 2 | P0 | Autocomplete UI | Reutilizar 15 itens (sem remove/mount) | 0 ‚Äústutter‚Äù ao digitar r√°pido | manual + profiler |
| 3 | P0 | Streaming | Buffer de chunks + flush 30‚Äì20fps; remover `+=` por delta | CPU est√°vel; menos GC | ChatPerf + throughput |
| 4 | P0 | Commands I/O | `to_thread` em `/read`, `/add`, `/image`, `/pdf` etc | UI n√£o congela em arquivo grande | teste manual + unit mocks |
| 5 | P1 | Startup | Bridge init em background + UI ‚Äúloading‚Äù | first paint < 200ms | medi√ß√£o `t_submit_to_worker` e tempo de mount |
| 6 | P1 | StatusBar | Remover exce√ß√µes em watchers (id inexistente) | 0 exce√ß√µes no hot path | pytest + log sem erros |
| 7 | P2 | InputArea | Otimizar detec√ß√£o/status | typing est√°vel em inputs grandes | benchmark local |

---

## 5) Plano cient√≠fico de valida√ß√£o (antes/depois)

### 5.1 M√©tricas m√≠nimas (sem novas deps)
1) **Lat√™ncia de streaming (j√° existe):** `VERTICE_TUI_PERF_LOG_PATH=/tmp/vertice_tui_perf.jsonl`
   - `t_submit_to_worker_ms` (p95)
   - `t_submit_to_first_text_delta_ms` (p95)
   - `t_total_ms`
2) **Autocomplete timing:** adicionar logging (debug/JSONL) para:
   - tempo de `get_completions` (ms)
   - tempo de atualiza√ß√£o do dropdown (ms)
3) **Estabilidade de sess√£o longa:** registrar:
   - `len(ResponseView.children)` (depois do trim)
   - tempo de `append_chunk` (ms) em p95/p99 (amostragem)

### 5.2 Benchmarks recomendados (scripts locais)
- **Typing benchmark:** simular 200‚Äì500 eventos Input.Changed com strings crescentes; medir p95.
- **Streaming benchmark:** stream artificial com 10k deltas pequenos vs 1k deltas maiores; comparar CPU/tempo total.
- **Scrollback benchmark:** montar 1k mensagens e confirmar que o trim mant√©m responsivo (limit 300).

### 5.3 Crit√©rios de aceita√ß√£o (pragm√°ticos)
- 0 congelamentos percept√≠veis durante:
  - digita√ß√£o r√°pida com autocomplete ligado
  - `/read` de arquivo ‚Äúgrande‚Äù (ex.: 1‚Äì5MB)
  - `/add src/**/*.py` (limitado a 10 arquivos)
- `t_submit_to_first_text_delta_ms` sem spikes (p99 consistente).

---

## 6) Pr√≥ximos passos (sequ√™ncia ideal de PRs)
1) PR-1 (P0): Autocomplete (debounce/offload + dropdown recycle) + benchmark typing.
2) PR-2 (P0): ResponseView streaming buffer/flush + benchmark streaming.
3) PR-3 (P0): `to_thread` para disk I/O em handlers (`/read`, `/add`, `/image`, `/pdf`) + smoke tests.
4) PR-4 (P1): Bridge init em background + m√©tricas de startup.
5) PR-5 (P1/P2): StatusBar watchers + InputArea micro-optimiza√ß√µes.
