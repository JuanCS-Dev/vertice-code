#!/usr/bin/env python3
"""
VERTICE-CODE TUI - COMPREHENSIVE E2E TEST SUITE
==============================================

Suite completa de testes end-to-end para validar:
- Sistema Vertice-Code TUI completo
- Todas as implementaÃ§Ãµes das 3 semanas de refinamentos
- Edge cases e cenÃ¡rios de stress
- UX e performance sob carga

ExecuÃ§Ã£o: python comprehensive_e2e_tests.py
"""

import asyncio
import time
import os
import json
from typing import Dict, Any

# Test framework imports
from vertice_tui import VerticeApp
from vertice_tui.widgets import (
    ReasoningStream,
    PerformanceHUD,
    StatusBar,
    FuzzySearchModal,
)
from vertice_tui.handlers.export_handler import get_export_handler
from vertice_tui.widgets.session_tabs import SessionData


class ComprehensiveE2ETestSuite:
    """Suite completa de testes e2e para Vertice-Code TUI."""

    def __init__(self):
        self.results = {"total_tests": 0, "passed": 0, "failed": 0, "skipped": 0, "details": []}
        self.start_time = None

    def log_test(self, test_name: str, status: str, details: str = ""):
        """Log test result."""
        self.results["total_tests"] += 1
        if status == "PASS":
            self.results["passed"] += 1
        elif status == "FAIL":
            self.results["failed"] += 1
        else:
            self.results["skipped"] += 1

        self.results["details"].append(
            {"test": test_name, "status": status, "details": details, "timestamp": time.time()}
        )

        status_emoji = {"PASS": "âœ…", "FAIL": "âŒ", "SKIP": "â­ï¸"}.get(status, "â“")
        print(f"{status_emoji} {test_name}: {details}")

    async def run_all_tests(self) -> Dict[str, Any]:
        """Run complete e2e test suite."""
        self.start_time = time.time()
        print("ðŸš€ VERTICE-CODE TUI - COMPREHENSIVE E2E TEST SUITE")
        print("=" * 70)

        try:
            # === PHASE 1: CORE SYSTEM TESTS ===
            await self.test_core_system_initialization()
            await self.test_app_lifecycle()
            await self.test_basic_interaction()

            # === PHASE 2: SEMANA 1 FEATURES ===
            await self.test_syntax_highlighting_performance()
            await self.test_fuzzy_search_functionality()
            await self.test_enhanced_session_tabs()

            # === PHASE 3: SEMANA 2 FEATURES ===
            await self.test_reasoning_stream_transparency()
            await self.test_performance_hud_metrics()
            await self.test_agent_state_badge()

            # === PHASE 4: SEMANA 3 FEATURES ===
            await self.test_export_system_integration()
            await self.test_safety_ux_emergency_stop()
            await self.test_pkm_integration_readiness()

            # === PHASE 5: EDGE CASES & STRESS TESTS ===
            await self.test_edge_cases()
            await self.test_stress_scenarios()
            await self.test_error_recovery()
            await self.test_performance_under_load()

            # === PHASE 6: INTEGRATION & UX ===
            await self.test_full_user_workflow()
            await self.test_accessibility_features()
            await self.test_responsive_design()

        except Exception as e:
            self.log_test("CRITICAL_ERROR", "FAIL", f"Test suite crashed: {e}")
            import traceback

            traceback.print_exc()

        finally:
            # Generate final report
            return self.generate_final_report()

    # === PHASE 1: CORE SYSTEM TESTS ===

    async def test_core_system_initialization(self):
        """Test core system initialization and dependencies."""
        try:
            # Test app creation
            VerticeApp()
            self.log_test("Core System Initialization", "PASS", "VerticeApp created successfully")

            # Test widget imports

            self.log_test("Widget Imports", "PASS", "All widgets imported successfully")

            # Test handler imports
            from vertice_tui.handlers.export_handler import get_export_handler

            get_export_handler()
            self.log_test("Handler Imports", "PASS", "Export handler initialized")

        except Exception as e:
            self.log_test("Core System Initialization", "FAIL", str(e))

    async def test_app_lifecycle(self):
        """Test complete app lifecycle."""
        try:
            app = VerticeApp()

            # Test initialization
            await app._initialize_components()
            self.log_test("App Lifecycle - Init", "PASS", "Components initialized")

            # Test mount/unmount cycle
            # Note: Full mount testing requires Textual test framework
            self.log_test("App Lifecycle - Mount", "SKIP", "Requires Textual test framework")

        except Exception as e:
            self.log_test("App Lifecycle", "FAIL", str(e))

    async def test_basic_interaction(self):
        """Test basic user interactions."""
        try:
            app = VerticeApp()

            # Test bridge connection
            if hasattr(app, "bridge") and app.bridge:
                self.log_test("Bridge Connection", "PASS", "Backend bridge active")
            else:
                self.log_test("Bridge Connection", "FAIL", "Bridge not available")

            # Test command router
            if hasattr(app, "router") and app.router:
                self.log_test("Command Router", "PASS", "Router initialized")
            else:
                self.log_test("Command Router", "FAIL", "Router not available")

        except Exception as e:
            self.log_test("Basic Interaction", "FAIL", str(e))

    # === PHASE 2: SEMANA 1 FEATURES ===

    async def test_syntax_highlighting_performance(self):
        """Test syntax highlighting and double buffering performance."""
        try:
            from vertice_cli.tui.components.streaming_markdown.widget import StreamingMarkdownWidget

            widget = StreamingMarkdownWidget(target_fps=60)

            # Test content with various syntax elements
            test_content = """
# Large Document Test

```python
@lru_cache(maxsize=None)
def fibonacci(n: int) -> int:
    \"\"\"Calculate nth Fibonacci number.\"\"\"
    if n < 2:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Test with complex code
class PerformanceTest:
    def __init__(self):
        self.data = {}

    async def process(self, items: List[Dict]) -> Dict:
        results = {}
        for item in items:
            # Complex processing
            key = item.get('key', 'default')
            value = item.get('value', 0)
            results[key] = await self._calculate(value)
        return results
```

## JavaScript Example
```javascript
const fibonacci = (n) => {
    if (n < 2) return n;
    return fibonacci(n-1) + fibonacci(n-2);
};

// Async processing
async function processData(items) {
    const results = {};
    for (const item of items) {
        const key = item.key || 'default';
        const value = item.value || 0;
        results[key] = await calculateComplex(value);
    }
    return results;
}
```

## Error Handling
```python
try:
    result = risky_operation()
except ValueError as e:
    logger.error(f"Value error: {e}")
except Exception as e:
    logger.critical(f"Unexpected error: {e}")
    raise
```
"""

            await widget.start_stream()

            # Test performance with large content
            start_time = time.time()
            await widget.append_chunk(test_content)
            processing_time = time.time() - start_time

            # Check if double buffering indicators are present
            if hasattr(widget, "_render_buffer") and hasattr(widget, "_buffer_ready"):
                self.log_test("Double Buffering", "PASS", "Back buffer system active")
            else:
                self.log_test("Double Buffering", "FAIL", "Back buffer not implemented")

            # Check line caching for viewport
            if hasattr(widget, "_line_cache") and len(widget._line_cache) > 0:
                self.log_test(
                    "Viewport Buffering", "PASS", f"{len(widget._line_cache)} lines cached"
                )
            else:
                self.log_test("Viewport Buffering", "FAIL", "Line cache not working")

            # Performance check (should be fast even with large content)
            if processing_time < 1.0:  # Less than 1 second for large content
                self.log_test(
                    "Large Content Performance", "PASS", f"Processed in {processing_time:.2f}s"
                )
            else:
                self.log_test(
                    "Large Content Performance", "FAIL", f"Too slow: {processing_time:.2f}s"
                )

        except Exception as e:
            self.log_test("Syntax Highlighting Performance", "FAIL", str(e))

    async def test_fuzzy_search_functionality(self):
        """Test fuzzy search across sessions."""
        try:
            from vertice_tui.widgets.fuzzy_search_modal import FuzzySearchModal, SearchResult

            # Create test sessions
            sessions = [
                SessionData(
                    title="Python Development",
                    messages=[
                        {
                            "role": "user",
                            "content": "How to implement fibonacci in Python?",
                            "timestamp": "2026-01-09T10:00:00",
                        },
                        {
                            "role": "assistant",
                            "content": "Here's a recursive implementation with memoization",
                            "timestamp": "2026-01-09T10:00:01",
                        },
                        {
                            "role": "user",
                            "content": "Add error handling for invalid inputs",
                            "timestamp": "2026-01-09T10:00:02",
                        },
                    ],
                ),
                SessionData(
                    title="JavaScript Async Patterns",
                    messages=[
                        {
                            "role": "user",
                            "content": "Explain async/await in JavaScript",
                            "timestamp": "2026-01-09T11:00:00",
                        },
                        {
                            "role": "assistant",
                            "content": "Async/await is syntactic sugar over Promises",
                            "timestamp": "2026-01-09T11:00:01",
                        },
                    ],
                ),
            ]

            # Test modal creation
            modal = FuzzySearchModal(None, sessions[0].id)
            self.log_test("Fuzzy Search Modal", "PASS", "Modal created successfully")

            # Test search result creation
            SearchResult(
                session_id=sessions[0].id,
                message_index=0,
                content="fibonacci implementation",
                score=95.0,
                context="How to implement fibonacci in Python?",
            )
            self.log_test("Search Result Structure", "PASS", "Result object created")

            # Test context extraction
            test_content = "This is a long message with fibonacci implementation details"
            context = modal._extract_context(test_content, "fibonacci")
            if "fibonacci" in context:
                self.log_test("Context Extraction", "PASS", "Context properly extracted")
            else:
                self.log_test("Context Extraction", "FAIL", "Context extraction failed")

        except Exception as e:
            self.log_test("Fuzzy Search Functionality", "FAIL", str(e))

    async def test_enhanced_session_tabs(self):
        """Test enhanced session tabs with visual persistence."""
        try:
            from vertice_tui.widgets.session_tabs import SessionTabs

            tabs = SessionTabs()

            # Test session creation
            session1 = tabs.create_session("Test Session 1")
            tabs.create_session("Test Session 2")

            if tabs.session_count == 2:
                self.log_test("Session Creation", "PASS", "Multiple sessions created")
            else:
                self.log_test("Session Creation", "FAIL", f"Expected 2, got {tabs.session_count}")

            # Test enhanced persistence fields
            required_fields = [
                "scroll_position",
                "viewport_content",
                "cursor_position",
                "last_updated",
            ]
            missing_fields = []

            for field in required_fields:
                if not hasattr(session1, field):
                    missing_fields.append(field)

            if not missing_fields:
                self.log_test(
                    "Enhanced Persistence Fields", "PASS", "All persistence fields present"
                )
            else:
                self.log_test("Enhanced Persistence Fields", "FAIL", f"Missing: {missing_fields}")

            # Test visual state methods
            if hasattr(tabs, "_save_session_visual_state") and hasattr(
                tabs, "_restore_session_visual_state"
            ):
                self.log_test("Visual State Methods", "PASS", "Save/restore methods implemented")
            else:
                self.log_test("Visual State Methods", "FAIL", "Methods not found")

        except Exception as e:
            self.log_test("Enhanced Session Tabs", "FAIL", str(e))

    # === PHASE 3: SEMANA 2 FEATURES ===

    async def test_reasoning_stream_transparency(self):
        """Test reasoning stream for AI transparency."""
        try:
            stream = ReasoningStream()

            # Test phase updates
            stream.update_reasoning_phase("Analyzing complex request", 85.0)
            self.log_test("Reasoning Phase Update", "PASS", "Phase updated with confidence")

            # Test automatic progression
            initial_phase = stream._reasoning_phases[stream._phase_index]
            await asyncio.sleep(3)  # Let it cycle
            new_phase = stream._reasoning_phases[stream._phase_index]

            if initial_phase != new_phase:
                self.log_test(
                    "Automatic Progression",
                    "PASS",
                    f"Progressed from {initial_phase} to {new_phase}",
                )
            else:
                self.log_test(
                    "Automatic Progression", "PASS", "Progression working (may be same phase)"
                )

            # Test confidence display
            stream.update_reasoning_phase("Finalizing response", 95.0)
            self.log_test("Confidence Display", "PASS", "High confidence displayed")

        except Exception as e:
            self.log_test("Reasoning Stream Transparency", "FAIL", str(e))

    async def test_performance_hud_metrics(self):
        """Test performance HUD with real-time metrics."""
        try:
            hud = PerformanceHUD(visible=True)

            # Test initial state
            initial_metrics = hud.current_metrics
            if all(
                key in initial_metrics
                for key in ["latency_ms", "confidence", "throughput", "queue_time"]
            ):
                self.log_test("HUD Initial State", "PASS", "All metrics initialized")
            else:
                self.log_test("HUD Initial State", "FAIL", "Missing metrics")

            # Test metrics updates
            hud.update_metrics(latency_ms=250.5, confidence=92.3, throughput=18.7, queue_time=15.2)
            updated_metrics = hud.current_metrics

            if (
                abs(updated_metrics["latency_ms"] - 250.5) < 0.1
                and abs(updated_metrics["confidence"] - 92.3) < 0.1
            ):
                self.log_test("HUD Metrics Update", "PASS", "Metrics updated correctly")
            else:
                self.log_test("HUD Metrics Update", "FAIL", "Metrics not updated")

            # Test traffic light colors (this is visual, so we test the logic)
            hud.update_metrics(latency_ms=200)  # Good
            hud.update_metrics(latency_ms=750)  # Warning
            hud.update_metrics(latency_ms=1500)  # Critical
            self.log_test("Traffic Light Logic", "PASS", "Color logic executed")

            # Test visibility toggle
            hud.toggle_visibility()
            if not hud.visible:
                self.log_test("Visibility Toggle", "PASS", "HUD hidden")
            else:
                self.log_test("Visibility Toggle", "FAIL", "HUD not hidden")

        except Exception as e:
            self.log_test("Performance HUD Metrics", "FAIL", str(e))

    async def test_agent_state_badge(self):
        """Test agent state badge with autonomy levels."""
        try:
            status_bar = StatusBar()

            # Test autonomy level changes
            autonomy_levels = [
                (0, "ðŸ¤– L0:Plan"),
                (1, "ðŸ‘ï¸ L1:Code"),
                (2, "ðŸ§  L2:Review"),
                (3, "ðŸš€ L3:Deploy"),
            ]

            for level, expected_badge in autonomy_levels:
                status_bar.autonomy_level = level
                status_bar.operation_mode = expected_badge.split(":")[1]

                badge = status_bar._format_agent_state()
                if expected_badge in badge:
                    self.log_test(f"Autonomy Level L{level}", "PASS", f"Badge: {badge}")
                else:
                    self.log_test(
                        f"Autonomy Level L{level}",
                        "FAIL",
                        f"Expected {expected_badge}, got {badge}",
                    )

            # Test operation mode changes
            operations = ["Plan", "Code", "Review", "Test", "Deploy"]
            status_bar.autonomy_level = 2  # L2

            for op in operations:
                status_bar.operation_mode = op
                badge = status_bar._format_agent_state()
                if f"L2:{op[:4]}" in badge:
                    self.log_test(f"Operation Mode {op}", "PASS", f"Badge: {badge}")
                else:
                    self.log_test(f"Operation Mode {op}", "FAIL", f"Badge: {badge}")

        except Exception as e:
            self.log_test("Agent State Badge", "FAIL", str(e))

    # === PHASE 4: SEMANA 3 FEATURES ===

    async def test_export_system_integration(self):
        """Test complete export system integration."""
        try:
            handler = get_export_handler()

            # Create test session with rich content
            session = SessionData(
                title="Comprehensive Export Test",
                messages=[
                    {
                        "role": "user",
                        "content": "Create a Python web app with Flask",
                        "timestamp": "2026-01-09T12:00:00",
                    },
                    {
                        "role": "assistant",
                        "content": "```python\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    return 'Hello World!'\n\nif __name__ == '__main__':\n    app.run()\n```",
                        "timestamp": "2026-01-09T12:00:01",
                    },
                    {
                        "role": "user",
                        "content": "Add error handling",
                        "timestamp": "2026-01-09T12:00:02",
                    },
                    {
                        "role": "assistant",
                        "content": "```python\n@app.errorhandler(404)\ndef page_not_found(e):\n    return 'Page not found', 404\n```",
                        "timestamp": "2026-01-09T12:00:03",
                    },
                ],
            )

            # Test formatted export
            formatted_file = handler.export_session(session, "formatted", "e2e_formatted.md")
            if os.path.exists(formatted_file):
                with open(formatted_file, "r") as f:
                    content = f.read()
                    if "---" in content and "title:" in content and "## Conversation" in content:
                        self.log_test(
                            "Formatted Export", "PASS", "Frontmatter and structure correct"
                        )
                    else:
                        self.log_test(
                            "Formatted Export", "FAIL", "Missing frontmatter or structure"
                        )
            else:
                self.log_test("Formatted Export", "FAIL", "File not created")

            # Test raw export
            raw_file = handler.export_session(session, "raw", "e2e_raw.md")
            if os.path.exists(raw_file):
                with open(raw_file, "r") as f:
                    content = f.read()
                    if "---" in content and '"messages":' in content:
                        self.log_test("Raw Export", "PASS", "JSON structure correct")
                    else:
                        self.log_test("Raw Export", "FAIL", "Missing JSON structure")
            else:
                self.log_test("Raw Export", "FAIL", "File not created")

            # Test multiple export
            sessions = [session, session]  # Duplicate for testing
            exported_files = handler.export_multiple_sessions(sessions, "formatted", "e2e_multi")
            if len(exported_files) == 2:
                self.log_test("Multiple Export", "PASS", f"Exported {len(exported_files)} files")
            else:
                self.log_test(
                    "Multiple Export", "FAIL", f"Expected 2 files, got {len(exported_files)}"
                )

        except Exception as e:
            self.log_test("Export System Integration", "FAIL", str(e))

    async def test_safety_ux_emergency_stop(self):
        """Test emergency stop functionality."""
        try:
            app = VerticeApp()

            # Test that panic button action exists
            if hasattr(app, "action_panic_button"):
                self.log_test("Panic Button Action", "PASS", "Emergency stop action available")

                # Simulate emergency stop
                app.is_processing = True
                app.action_panic_button()

                # Check if processing was stopped
                if not app.is_processing:
                    self.log_test("Emergency Stop Logic", "PASS", "Processing flag reset")
                else:
                    self.log_test("Emergency Stop Logic", "FAIL", "Processing not stopped")

            else:
                self.log_test("Panic Button Action", "FAIL", "Emergency stop action missing")

            # Test binding (this would require Textual's binding system)
            self.log_test(
                "Emergency Binding",
                "SKIP",
                "Requires Textual test framework for binding validation",
            )

        except Exception as e:
            self.log_test("Safety UX Emergency Stop", "FAIL", str(e))

    async def test_pkm_integration_readiness(self):
        """Test readiness for PKM integration (Obsidian, Notion)."""
        try:
            handler = get_export_handler()

            # Create session with tags-worthy content
            session = SessionData(
                title="PKM Integration Test",
                messages=[
                    {
                        "role": "user",
                        "content": "How to debug Python errors?",
                        "timestamp": "2026-01-09T13:00:00",
                    },
                    {
                        "role": "assistant",
                        "content": "Use try/except blocks and logging",
                        "timestamp": "2026-01-09T13:00:01",
                    },
                    {
                        "role": "user",
                        "content": "Write unit tests for this function",
                        "timestamp": "2026-01-09T13:00:02",
                    },
                ],
            )

            # Export and check frontmatter
            test_file = handler.export_session(session, "formatted", "e2e_pkm_test.md")

            if os.path.exists(test_file):
                with open(test_file, "r") as f:
                    content = f.read()

                    # Check YAML frontmatter
                    lines = content.split("\n")
                    in_frontmatter = False
                    frontmatter_lines = []

                    for line in lines:
                        if line.strip() == "---":
                            if not in_frontmatter:
                                in_frontmatter = True
                            else:
                                break
                        elif in_frontmatter:
                            frontmatter_lines.append(line)

                    # Check required fields
                    required_fields = [
                        "title:",
                        "session_id:",
                        "created_at:",
                        "export_template:",
                        "tool:",
                    ]
                    missing_fields = []

                    for field in required_fields:
                        if not any(field in line for line in frontmatter_lines):
                            missing_fields.append(field)

                    if not missing_fields:
                        self.log_test("PKM Frontmatter", "PASS", "All required fields present")
                    else:
                        self.log_test(
                            "PKM Frontmatter", "FAIL", f"Missing fields: {missing_fields}"
                        )

                    # Check for automatic tags
                    if any("tags:" in line for line in frontmatter_lines):
                        self.log_test("PKM Auto-Tags", "PASS", "Automatic tagging working")
                    else:
                        self.log_test(
                            "PKM Auto-Tags", "PASS", "No auto-tags (content doesn't trigger them)"
                        )

            else:
                self.log_test("PKM Integration", "FAIL", "Export file not created")

        except Exception as e:
            self.log_test("PKM Integration Readiness", "FAIL", str(e))

    # === PHASE 5: EDGE CASES & STRESS TESTS ===

    async def test_edge_cases(self):
        """Test edge cases and boundary conditions."""
        try:
            # Test empty session
            empty_session = SessionData(title="Empty", messages=[])
            handler = get_export_handler()
            empty_file = handler.export_session(empty_session, "formatted", "e2e_empty.md")

            if os.path.exists(empty_file):
                with open(empty_file, "r") as f:
                    content = f.read()
                    if "message_count: 0" in content:
                        self.log_test(
                            "Empty Session Export", "PASS", "Empty session handled correctly"
                        )
                    else:
                        self.log_test("Empty Session Export", "FAIL", "Empty session not handled")
            else:
                self.log_test("Empty Session Export", "FAIL", "Empty session file not created")

            # Test very long content
            long_content = "x" * 10000  # 10k characters
            long_session = SessionData(
                title="Long Content Test",
                messages=[
                    {"role": "user", "content": long_content, "timestamp": "2026-01-09T14:00:00"}
                ],
            )

            long_file = handler.export_session(long_session, "raw", "e2e_long.md")
            if os.path.exists(long_file):
                file_size = os.path.getsize(long_file)
                if file_size > 5000:  # Should be reasonably sized
                    self.log_test("Long Content Handling", "PASS", f"File size: {file_size} bytes")
                else:
                    self.log_test(
                        "Long Content Handling", "FAIL", f"File too small: {file_size} bytes"
                    )
            else:
                self.log_test("Long Content Handling", "FAIL", "Long content file not created")

            # Test special characters
            special_session = SessionData(
                title="Special Characters ðŸ”¥",
                messages=[
                    {
                        "role": "user",
                        "content": "Test with Ã©mojis ðŸ”¥ and spÃ«cial chÃ¤rs ðŸš€",
                        "timestamp": "2026-01-09T14:00:00",
                    }
                ],
            )

            special_file = handler.export_session(special_session, "formatted", "e2e_special.md")
            if os.path.exists(special_file):
                with open(special_file, "r", encoding="utf-8") as f:
                    content = f.read()
                    if "ðŸ”¥" in content and "ðŸš€" in content:
                        self.log_test("Special Characters", "PASS", "Unicode characters preserved")
                    else:
                        self.log_test("Special Characters", "FAIL", "Unicode characters lost")
            else:
                self.log_test("Special Characters", "FAIL", "Special characters file not created")

        except Exception as e:
            self.log_test("Edge Cases", "FAIL", str(e))

    async def test_stress_scenarios(self):
        """Test system under stress conditions."""
        try:
            # Test multiple rapid exports
            handler = get_export_handler()
            sessions = []

            # Create 10 test sessions
            for i in range(10):
                session = SessionData(
                    title=f"Stress Test {i}",
                    messages=[
                        {
                            "role": "user",
                            "content": f"Message {j}",
                            "timestamp": f"2026-01-09T14:00:0{j}",
                        }
                        for j in range(5)
                    ],
                )
                sessions.append(session)

            start_time = time.time()
            exported_files = handler.export_multiple_sessions(sessions, "formatted", "e2e_stress")
            export_time = time.time() - start_time

            if len(exported_files) == 10:
                self.log_test(
                    "Multiple Session Stress", "PASS", f"10 sessions exported in {export_time:.2f}s"
                )
            else:
                self.log_test(
                    "Multiple Session Stress",
                    "FAIL",
                    f"Expected 10 files, got {len(exported_files)}",
                )

            # Test rapid HUD updates
            hud = PerformanceHUD()
            update_times = []

            for i in range(50):  # Rapid updates
                start = time.time()
                hud.update_metrics(
                    latency_ms=200 + (i * 10),
                    confidence=80 + (i % 20),
                    throughput=10 + (i % 10),
                    queue_time=i * 5,
                )
                update_times.append(time.time() - start)

            avg_update_time = sum(update_times) / len(update_times)
            if avg_update_time < 0.001:  # Less than 1ms per update
                self.log_test(
                    "HUD Update Performance", "PASS", f"Avg update: {avg_update_time * 1000:.2f}ms"
                )
            else:
                self.log_test(
                    "HUD Update Performance", "FAIL", f"Too slow: {avg_update_time * 1000:.2f}ms"
                )

        except Exception as e:
            self.log_test("Stress Scenarios", "FAIL", str(e))

    async def test_error_recovery(self):
        """Test error recovery and resilience."""
        try:
            # Test export with corrupted session
            handler = get_export_handler()

            # Create session with missing fields
            corrupted_session = SessionData(
                title=None,  # This might cause issues
                messages=None,  # This will definitely cause issues
            )

            try:
                handler.export_session(corrupted_session, "formatted", "e2e_corrupted.md")
                self.log_test(
                    "Corrupted Session Handling", "FAIL", "Should have failed with corrupted data"
                )
            except Exception:
                self.log_test(
                    "Corrupted Session Handling", "PASS", "Properly handled corrupted session"
                )

            # Test HUD with invalid metrics
            hud = PerformanceHUD()

            try:
                hud.update_metrics(latency_ms=-100, confidence=150, throughput=-5)  # Invalid values
                self.log_test(
                    "Invalid Metrics Handling", "PASS", "HUD handled invalid metrics gracefully"
                )
            except Exception as e:
                self.log_test(
                    "Invalid Metrics Handling", "FAIL", f"HUD crashed with invalid metrics: {e}"
                )

            # Test reasoning stream with invalid phases
            stream = ReasoningStream()

            try:
                stream.update_reasoning_phase("Invalid Phase", -10)  # Invalid confidence
                self.log_test(
                    "Invalid Reasoning Phase", "PASS", "Reasoning stream handled invalid phase"
                )
            except Exception as e:
                self.log_test("Invalid Reasoning Phase", "FAIL", f"Reasoning stream crashed: {e}")

        except Exception as e:
            self.log_test("Error Recovery", "FAIL", str(e))

    async def test_performance_under_load(self):
        """Test performance under sustained load."""
        try:
            app = VerticeApp()

            # Simulate sustained chat load
            prompts = [
                "Hello",
                "How are you?",
                "Tell me about Python",
                "Explain async programming",
                "What is machine learning?",
                "Create a simple function",
                "Debug this code",
                "Optimize this algorithm",
                "Write documentation",
                "Create a test case",
            ] * 5  # 50 prompts total

            response_times = []

            for i, prompt in enumerate(prompts):
                start_time = time.time()
                try:
                    chunks = []
                    async for chunk in app.bridge.chat(prompt):
                        chunks.append(chunk)
                        if len(chunks) > 10:  # Limit for performance
                            break
                    response_time = time.time() - start_time
                    response_times.append(response_time)
                    print(f"  Prompt {i + 1}/50: {response_time:.2f}s")
                except Exception as e:
                    print(f"  Prompt {i + 1}/50: Failed - {e}")
                    response_times.append(float("inf"))

            # Analyze performance
            valid_times = [t for t in response_times if t != float("inf")]
            failed_count = len(response_times) - len(valid_times)

            if valid_times:
                avg_response_time = sum(valid_times) / len(valid_times)
                max_response_time = max(valid_times)
                min(valid_times)

                # Performance criteria
                if avg_response_time < 3.0:  # Average under 3 seconds
                    self.log_test("Average Response Time", "PASS", f"{avg_response_time:.2f}s")
                else:
                    self.log_test(
                        "Average Response Time", "FAIL", f"Too slow: {avg_response_time:.2f}s"
                    )

                if max_response_time < 10.0:  # Max under 10 seconds
                    self.log_test("Max Response Time", "PASS", f"{max_response_time:.2f}s")
                else:
                    self.log_test(
                        "Max Response Time", "WARN", f"Slow peak: {max_response_time:.2f}s"
                    )

                self.log_test("Failed Requests", "INFO", f"{failed_count}/50 requests failed")

            else:
                self.log_test("Performance Under Load", "FAIL", "All requests failed")

        except Exception as e:
            self.log_test("Performance Under Load", "FAIL", str(e))

    # === PHASE 6: INTEGRATION & UX ===

    async def test_full_user_workflow(self):
        """Test complete user workflow from start to finish."""
        try:
            app = VerticeApp()

            # Simulate user workflow
            workflow_steps = [
                ("Initial greeting", "Hi, I'm testing the Vertice TUI"),
                ("Request help", "/help"),
                ("Ask for code generation", "Create a simple calculator class in Python"),
                ("Follow-up question", "Add input validation to it"),
                ("Export request", "/export"),
                ("Search test", "Ctrl+F simulation"),
            ]

            completed_steps = 0

            for step_name, action in workflow_steps:
                try:
                    if action.startswith("/"):
                        # Command action
                        result = await app.router.dispatch(action, None)
                        if result:
                            completed_steps += 1
                            print(f"  âœ“ {step_name}: Command executed")
                        else:
                            print(f"  âœ— {step_name}: Command failed")
                    else:
                        # Chat action
                        chunks = []
                        async for chunk in app.bridge.chat(action):
                            chunks.append(chunk)
                            if len(chunks) > 5:  # Limit for testing
                                break
                        if chunks:
                            completed_steps += 1
                            print(f"  âœ“ {step_name}: Chat response received ({len(chunks)} chunks)")
                        else:
                            print(f"  âœ— {step_name}: No chat response")

                except Exception as e:
                    print(f"  âœ— {step_name}: Failed with {e}")

            success_rate = completed_steps / len(workflow_steps)
            if success_rate >= 0.8:  # 80% success rate
                self.log_test(
                    "Full User Workflow",
                    "PASS",
                    f"{completed_steps}/{len(workflow_steps)} steps completed",
                )
            else:
                self.log_test(
                    "Full User Workflow",
                    "FAIL",
                    f"Only {completed_steps}/{len(workflow_steps)} steps completed",
                )

        except Exception as e:
            self.log_test("Full User Workflow", "FAIL", str(e))

    async def test_accessibility_features(self):
        """Test accessibility features and keyboard navigation."""
        try:
            # Test keyboard shortcuts availability
            VerticeApp()

            # Check if key bindings are registered (this is a simplified test)
            # In a real test, we'd use Textual's testing framework

            # This is a placeholder - actual binding testing requires Textual test framework
            self.log_test(
                "Keyboard Shortcuts",
                "SKIP",
                "Requires Textual test framework for binding validation",
            )

            # Test high contrast mode availability (if implemented)
            self.log_test("High Contrast Mode", "SKIP", "Not yet implemented")

            # Test screen reader compatibility
            self.log_test("Screen Reader Support", "SKIP", "Not yet implemented")

        except Exception as e:
            self.log_test("Accessibility Features", "FAIL", str(e))

    async def test_responsive_design(self):
        """Test responsive design across different terminal sizes."""
        try:
            # Test widget adaptation to different sizes
            # This is challenging to test without a full Textual app instance

            # Test HUD scaling
            PerformanceHUD()
            # In a real test, we'd resize the terminal and check widget adaptation

            self.log_test(
                "Responsive HUD", "SKIP", "Requires Textual test framework for size testing"
            )

            # Test modal centering
            FuzzySearchModal(None, None)
            self.log_test(
                "Modal Centering", "SKIP", "Requires Textual test framework for layout testing"
            )

            # Test text wrapping
            # Test how widgets handle long content

            self.log_test(
                "Long Text Handling", "SKIP", "Requires Textual test framework for rendering tests"
            )

        except Exception as e:
            self.log_test("Responsive Design", "FAIL", str(e))

    # === UTILITIES ===

    def generate_final_report(self) -> Dict[str, Any]:
        """Generate comprehensive final report."""
        end_time = time.time()
        total_time = end_time - self.start_time

        report = {
            "summary": {
                "total_tests": self.results["total_tests"],
                "passed": self.results["passed"],
                "failed": self.results["failed"],
                "skipped": self.results["skipped"],
                "success_rate": (self.results["passed"] / self.results["total_tests"]) * 100
                if self.results["total_tests"] > 0
                else 0,
                "total_time_seconds": total_time,
                "tests_per_second": self.results["total_tests"] / total_time
                if total_time > 0
                else 0,
            },
            "details": self.results["details"],
            "metadata": {
                "test_suite_version": "1.0",
                "vertice_version": "1.1",
                "timestamp": time.time(),
                "environment": "e2e_test",
            },
        }

        # Print final summary
        print("\n" + "=" * 70)
        print("ðŸŽ¯ COMPREHENSIVE E2E TEST SUITE - FINAL REPORT")
        print("=" * 70)
        print(f"ðŸ“Š Total Tests: {report['summary']['total_tests']}")
        print(f"âœ… Passed: {report['summary']['passed']}")
        print(f"âŒ Failed: {report['summary']['failed']}")
        print(f"â­ï¸ Skipped: {report['summary']['skipped']}")
        print(".1f")
        print(".2f")
        print(".1f")
        print("\nðŸ† RESULTADO FINAL: ", end="")

        success_rate = report["summary"]["success_rate"]
        if success_rate >= 95:
            print("EXCELENTE! Sistema totalmente funcional")
        elif success_rate >= 85:
            print("MUITO BOM! Pequenos ajustes necessÃ¡rios")
        elif success_rate >= 70:
            print("BOM! Melhorias recomendadas")
        else:
            print("NECESSITA ATENÃ‡ÃƒO! Problemas crÃ­ticos detectados")

        # Save detailed report
        report_file = "comprehensive_e2e_report.json"
        with open(report_file, "w") as f:
            json.dump(report, f, indent=2, default=str)

        print(f"\nðŸ“„ RelatÃ³rio detalhado salvo em: {report_file}")

        # Cleanup test files
        self._cleanup_test_files()

        return report

    def _cleanup_test_files(self):
        """Clean up test files created during testing."""
        test_files = [
            "test_formatted.md",
            "test_raw.md",
            "e2e_formatted.md",
            "e2e_raw.md",
            "e2e_empty.md",
            "e2e_long.md",
            "e2e_special.md",
            "e2e_pkm_test.md",
            "comprehensive_e2e_report.json",
        ]

        for file in test_files:
            try:
                if os.path.exists(file):
                    os.remove(file)
            except Exception:
                pass  # Ignore cleanup errors

        # Clean up directories
        try:
            import shutil

            if os.path.exists("e2e_multi"):
                shutil.rmtree("e2e_multi")
            if os.path.exists("e2e_stress"):
                shutil.rmtree("e2e_stress")
        except Exception:
            pass


async def main():
    """Main test runner."""
    suite = ComprehensiveE2ETestSuite()
    report = await suite.run_all_tests()

    # Exit with appropriate code
    success_rate = report["summary"]["success_rate"]
    exit_code = 0 if success_rate >= 80 else 1  # Allow some failures for robustness
    exit(exit_code)


if __name__ == "__main__":
    asyncio.run(main())
