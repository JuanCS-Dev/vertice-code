"""
PerformanceAgent: Comprehensive Test Suite (18+ tests).

Coverage Areas:
    1. Algorithmic Complexity Detection (5 tests)
    2. N+1 Query Detection (3 tests)
    3. Memory Pattern Analysis (3 tests)
    4. Performance Anti-patterns (3 tests)
    5. Performance Scoring (2 tests)
    6. Edge Cases (2 tests)

Philosophy (Boris Cherny):
    "Performance tests must measure real bottlenecks, not toy examples."
"""

import tempfile
from pathlib import Path
from unittest.mock import AsyncMock, MagicMock

import pytest

from vertice_core.agents.base import AgentTask
from vertice_core.agents.performance import (
    BottleneckType,
    ComplexityLevel,
    PerformanceAgent,
)


@pytest.fixture
def mock_llm():
    """Mock LLM client."""
    llm = MagicMock()
    llm.generate = AsyncMock(return_value="Performance analysis complete")
    return llm


@pytest.fixture
def performance_agent(mock_llm):
    """Create PerformanceAgent instance."""
    mock_mcp = MagicMock()
    return PerformanceAgent(llm_client=mock_llm, mcp_client=mock_mcp)


@pytest.fixture
def temp_project():
    """Create temporary project directory."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


# ============================================================================
# ALGORITHMIC COMPLEXITY DETECTION (5 tests)
# ============================================================================


@pytest.mark.asyncio
class TestAlgorithmicComplexity:
    """Test algorithmic complexity detection."""

    async def test_nested_loop_o_n2_detection(self, performance_agent, temp_project):
        """Test O(n²) nested loop detection."""
        test_file = temp_project / "nested.py"
        test_file.write_text(
            """
def find_duplicates(items):
    duplicates = []
    for i in items:
        for j in items:
            if i == j:
                duplicates.append(i)
    return duplicates
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        bottlenecks = response.data["bottlenecks"]
        assert len(bottlenecks) > 0

        # Find O(n²) bottleneck
        on2_found = any(
            b["type"] == BottleneckType.ALGORITHMIC_COMPLEXITY
            and b["complexity"] == ComplexityLevel.O_N2
            for b in bottlenecks
        )
        assert on2_found, "O(n²) nested loop not detected"

    async def test_nested_loop_o_n3_detection(self, performance_agent, temp_project):
        """Test O(n³) triple nested loop detection."""
        test_file = temp_project / "triple_nested.py"
        test_file.write_text(
            """
def triple_loop(data):
    result = []
    for i in data:
        for j in data:
            for k in data:
                result.append(i + j + k)
    return result
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        bottlenecks = response.data["bottlenecks"]

        # Find O(n³) bottleneck
        on3_found = any(
            b["type"] == BottleneckType.ALGORITHMIC_COMPLEXITY
            and b["complexity"] == ComplexityLevel.O_N3
            and b["severity"] == "critical"
            for b in bottlenecks
        )
        assert on3_found, "O(n³) triple loop not detected as critical"

    async def test_single_loop_no_false_positive(self, performance_agent, temp_project):
        """Test that single loops don't trigger complexity warnings."""
        test_file = temp_project / "single_loop.py"
        test_file.write_text(
            """
def process_items(items):
    result = []
    for item in items:
        result.append(item * 2)
    return result
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        bottlenecks = response.data["bottlenecks"]

        # Should not detect complexity issue for single loop
        complexity_issues = [
            b for b in bottlenecks if b["type"] == BottleneckType.ALGORITHMIC_COMPLEXITY
        ]
        assert len(complexity_issues) == 0, "False positive on single loop"

    async def test_comprehension_vs_nested_loop(self, performance_agent, temp_project):
        """Test detection suggests list comprehension."""
        test_file = temp_project / "comprehension_opportunity.py"
        test_file.write_text(
            """
def transform(matrix):
    result = []
    for row in matrix:
        for col in row:
            result.append(col * 2)
    return result
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        bottlenecks = response.data["bottlenecks"]

        # Should detect nested loop
        nested = any(b["type"] == BottleneckType.ALGORITHMIC_COMPLEXITY for b in bottlenecks)
        assert nested, "Nested loop not detected"

        # Should suggest optimization
        suggestions = any(
            "set operations" in b.get("optimization", "").lower()
            or "dict lookups" in b.get("optimization", "").lower()
            or "vectorization" in b.get("optimization", "").lower()
            for b in bottlenecks
        )
        assert suggestions, "No optimization suggestion provided"

    async def test_complexity_severity_scoring(self, performance_agent, temp_project):
        """Test that complexity severity is correctly assigned."""
        # Create file with both O(n²) and O(n³)
        test_file = temp_project / "mixed_complexity.py"
        test_file.write_text(
            """
def on2_function(data):
    for i in data:
        for j in data:
            pass

def on3_function(data):
    for i in data:
        for j in data:
            for k in data:
                pass
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        bottlenecks = response.data["bottlenecks"]

        # Find O(n²) - should be "high"
        on2 = [b for b in bottlenecks if b.get("complexity") == ComplexityLevel.O_N2]
        if on2:
            assert on2[0]["severity"] == "high"

        # Find O(n³) - should be "critical"
        on3 = [b for b in bottlenecks if b.get("complexity") == ComplexityLevel.O_N3]
        if on3:
            assert on3[0]["severity"] == "critical"


# ============================================================================
# N+1 QUERY DETECTION (3 tests)
# ============================================================================


@pytest.mark.asyncio
class TestNPlusOneDetection:
    """Test N+1 query pattern detection."""

    async def test_query_in_for_loop(self, performance_agent, temp_project):
        """Test detection of .get() in for loop."""
        test_file = temp_project / "n_plus_one.py"
        test_file.write_text(
            """
def get_users_with_profiles(user_ids):
    users = []
    for user_id in user_ids:
        user = User.objects.get(id=user_id)
        users.append(user)
    return users
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        bottlenecks = response.data["bottlenecks"]

        # Find N+1 issue
        n_plus_one = any(b["type"] == BottleneckType.N_PLUS_ONE_QUERY for b in bottlenecks)
        assert n_plus_one, "N+1 query not detected"

        # Should be high severity
        issue = next(b for b in bottlenecks if b["type"] == BottleneckType.N_PLUS_ONE_QUERY)
        assert issue["severity"] == "high"

    async def test_filter_in_loop(self, performance_agent, temp_project):
        """Test detection of .filter() in loop."""
        test_file = temp_project / "filter_in_loop.py"
        test_file.write_text(
            """
def load_related_data(items):
    results = []
    for item in items:
        related = RelatedModel.objects.filter(parent_id=item.id)
        results.extend(related)
    return results
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        bottlenecks = response.data["bottlenecks"]

        n_plus_one = any(b["type"] == BottleneckType.N_PLUS_ONE_QUERY for b in bottlenecks)
        assert n_plus_one, ".filter() in loop not detected"

    async def test_n_plus_one_suggests_prefetch(self, performance_agent, temp_project):
        """Test that N+1 detection suggests select_related/prefetch_related."""
        test_file = temp_project / "need_prefetch.py"
        test_file.write_text(
            """
def get_posts_with_authors(post_ids):
    posts = []
    for post_id in post_ids:
        post = Post.objects.get(id=post_id)
        posts.append(post)
    return posts
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        bottlenecks = response.data["bottlenecks"]

        # Find optimization suggestion
        optimization = next(
            (
                b["optimization"]
                for b in bottlenecks
                if b["type"] == BottleneckType.N_PLUS_ONE_QUERY
            ),
            "",
        )

        assert (
            "select_related" in optimization.lower()
            or "prefetch_related" in optimization.lower()
            or "batch" in optimization.lower()
        ), "Missing prefetch/batch suggestion"


# ============================================================================
# MEMORY PATTERN ANALYSIS (3 tests)
# ============================================================================


@pytest.mark.asyncio
class TestMemoryPatterns:
    """Test memory usage pattern analysis."""

    async def test_unbounded_list_growth(self, performance_agent, temp_project):
        """Test detection of unbounded list append in loop."""
        test_file = temp_project / "memory_leak.py"
        test_file.write_text(
            """
def process_large_file(filename):
    results = []
    for line in open(filename):
        results.append(line.strip())
    return results
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        bottlenecks = response.data["bottlenecks"]

        memory_issue = any(b["type"] == BottleneckType.MEMORY_LEAK for b in bottlenecks)
        assert memory_issue, "Unbounded list growth not detected"

    async def test_generator_suggestion(self, performance_agent, temp_project):
        """Test that memory issues suggest generators."""
        test_file = temp_project / "need_generator.py"
        test_file.write_text(
            """
def read_huge_dataset():
    data = []
    for i in range(1000000):
        data.append(process(i))
    return data
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        bottlenecks = response.data["bottlenecks"]

        if any(b["type"] == BottleneckType.MEMORY_LEAK for b in bottlenecks):
            optimization = next(
                b["optimization"] for b in bottlenecks if b["type"] == BottleneckType.MEMORY_LEAK
            )
            assert (
                "generator" in optimization.lower() or "yield" in optimization.lower()
            ), "Missing generator suggestion"

    async def test_no_false_positive_with_break(self, performance_agent, temp_project):
        """Test that loops with break conditions don't trigger false positives."""
        test_file = temp_project / "bounded_loop.py"
        test_file.write_text(
            """
def find_first_match(items):
    results = []
    for item in items:
        if item.matches():
            results.append(item)
            break
    return results
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        # Note: Current implementation might still flag this
        # In production, AST analysis would detect the break
        # This test documents expected behavior


# ============================================================================
# PERFORMANCE ANTI-PATTERNS (3 tests)
# ============================================================================


@pytest.mark.asyncio
class TestAntiPatterns:
    """Test performance anti-pattern detection."""

    async def test_string_concatenation_in_loop(self, performance_agent, temp_project):
        """Test detection of string += in loop."""
        test_file = temp_project / "string_concat.py"
        test_file.write_text(
            """
def build_html(items):
    html = ""
    for item in items:
        html += f"<li>{item}</li>"
    return html
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        bottlenecks = response.data["bottlenecks"]

        string_concat = any(b["type"] == BottleneckType.STRING_CONCAT for b in bottlenecks)
        assert string_concat, "String concatenation in loop not detected"

    async def test_uncompiled_regex_in_loop(self, performance_agent, temp_project):
        """Test detection of re.match/search in loop."""
        test_file = temp_project / "regex_in_loop.py"
        test_file.write_text(
            """
import re

def validate_emails(emails):
    valid = []
    for email in emails:
        if re.match(r"[^@]+@[^@]+", email):
            valid.append(email)
    return valid
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        bottlenecks = response.data["bottlenecks"]

        regex_issue = any(b["type"] == BottleneckType.REGEX_COMPILE for b in bottlenecks)
        assert regex_issue, "Uncompiled regex in loop not detected"

    async def test_optimization_suggests_precompile(self, performance_agent, temp_project):
        """Test that regex issues suggest pre-compilation."""
        test_file = temp_project / "need_compile.py"
        test_file.write_text(
            """
import re

def filter_patterns(lines):
    matches = []
    for line in lines:
        if re.search(r"\\d{3}-\\d{4}", line):
            matches.append(line)
    return matches
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        bottlenecks = response.data["bottlenecks"]

        if any(b["type"] == BottleneckType.REGEX_COMPILE for b in bottlenecks):
            optimization = next(
                b["optimization"] for b in bottlenecks if b["type"] == BottleneckType.REGEX_COMPILE
            )
            assert "compile" in optimization.lower(), "Missing compile suggestion"


# ============================================================================
# PERFORMANCE SCORING (2 tests)
# ============================================================================


@pytest.mark.asyncio
class TestPerformanceScoring:
    """Test performance score calculation."""

    async def test_clean_code_perfect_score(self, performance_agent, temp_project):
        """Test that clean code gets 100/100 score."""
        test_file = temp_project / "clean.py"
        test_file.write_text(
            """
def process_data(items):
    return [item * 2 for item in items]

def calculate_sum(numbers):
    return sum(numbers)
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        score = response.data["performance_score"]
        assert score >= 95, f"Clean code should score near 100, got {score}"

    async def test_score_decreases_with_issues(self, performance_agent, temp_project):
        """Test that score decreases with more issues."""
        test_file = temp_project / "problematic.py"
        test_file.write_text(
            """
def bad_performance(data):
    # O(n³) - critical
    for i in data:
        for j in data:
            for k in data:
                pass

    # N+1 query - high
    results = []
    for item in data:
        result = db.get(item.id)
        results.append(result)

    # String concat - medium
    html = ""
    for item in data:
        html += str(item)

    return html
"""
        )

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        score = response.data["performance_score"]
        assert score < 70, f"Problematic code should score low, got {score}"


# ============================================================================
# EDGE CASES (2 tests)
# ============================================================================


@pytest.mark.asyncio
class TestEdgeCases:
    """Test edge cases and error handling."""

    async def test_empty_directory(self, performance_agent, temp_project):
        """Test analysis of empty directory."""
        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        assert response.success
        assert response.data["files_analyzed"] == 0
        assert response.data["performance_score"] == 100  # No issues = perfect

    async def test_syntax_error_file_graceful_handling(self, performance_agent, temp_project):
        """Test that syntax errors don't crash the agent."""
        bad_file = temp_project / "syntax_error.py"
        bad_file.write_text("def broken(\n    this is invalid python")

        task = AgentTask(
            request="Analyze performance",
            context={"root_dir": str(temp_project)},
        )

        response = await performance_agent.execute(task)

        # Should not crash
        assert response.success or response.error is not None
        # Should skip the bad file
        assert "syntax_error.py" not in str(response.data.get("bottlenecks", []))
