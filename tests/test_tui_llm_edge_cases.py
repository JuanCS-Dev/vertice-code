"""
ðŸ§ª TUI + LLM Edge Cases - Scientific Real-World Testing

Tests TUI components with REAL LLM calls using our 3-provider failover:
1. Ollama (LOCAL - primary, no network needed)
2. Nebius (ONLINE - backup)
3. HuggingFace (ONLINE - fallback)

Philosophy:
- Test REAL failures, not mocks
- Verify failover works
- Measure response times
- Test concurrent streams
- Validate UI updates with real data
"""

import pytest
import asyncio
import time
from io import StringIO
from rich.console import Console

from vertice_core.core.llm import LLMClient
from vertice_core.tui.wisdom import WisdomSystem


class TestLLMEdgeCases:
    """Scientific edge case testing with real LLM."""

    @pytest.mark.asyncio
    async def test_llm_stream_renders_progressively(self):
        """Test: TUI updates as LLM streams (real-time rendering)."""
        console = Console(file=StringIO())
        messages = []

        # Create LLM client with failover
        llm = LLMClient()

        # Stream response and collect chunks
        prompt = "Say 'Hello World' in one word"
        start_time = time.time()

        async for chunk in llm.stream_chat(prompt=prompt):
            messages.append(chunk)
            # Simulate TUI update
            if len(messages) % 5 == 0:  # Every 5 chunks
                progress = len(messages)
                console.print(f"Streaming... {progress} chunks")

        elapsed = time.time() - start_time

        # Assertions
        assert len(messages) > 0, "Should receive stream chunks"
        assert elapsed < 30, f"Should complete in <30s, took {elapsed:.2f}s"

        full_response = "".join(messages)
        assert len(full_response) > 0, "Should have content"

        print(f"\nâœ… Streamed {len(messages)} chunks in {elapsed:.2f}s")
        print(f"   Response: {full_response[:100]}...")

    @pytest.mark.asyncio
    async def test_llm_failover_resilience(self):
        """Test: Falls back through providers when one fails."""
        llm = LLMClient()

        # Force provider priority
        llm.provider_priority = ["ollama", "nebius", "huggingface"]

        start_time = time.time()

        try:
            # Should try ollama first, failover if needed
            response = ""
            async for chunk in llm.stream_chat(prompt="Echo: test", enable_failover=True):
                response += chunk

            elapsed = time.time() - start_time

            assert len(response) > 0, "Should get response despite failovers"
            assert elapsed < 60, f"Failover should be fast, took {elapsed:.2f}s"

            # Check metrics
            stats = llm.get_metrics()
            print("\nâœ… Failover successful:")
            print(f"   Providers tried: {stats.get('providers', {})}")
            print(f"   Success rate: {stats.get('success_rate', 'N/A')}")
            print(f"   Response: {response[:50]}...")

        except Exception as e:
            pytest.fail(f"Failover should handle errors gracefully: {e}")

    @pytest.mark.asyncio
    async def test_concurrent_llm_streams(self):
        """Test: Multiple LLM streams run concurrently without blocking."""
        llm = LLMClient()

        prompts = ["Count: 1", "Count: 2", "Count: 3"]

        start_time = time.time()

        # Run 3 streams concurrently
        tasks = []
        for prompt in prompts:

            async def stream_prompt(p):
                chunks = []
                async for chunk in llm.stream_chat(prompt=p):
                    chunks.append(chunk)
                return "".join(chunks)

            tasks.append(stream_prompt(prompt))

        responses = await asyncio.gather(*tasks)
        elapsed = time.time() - start_time

        # Assertions
        assert len(responses) == 3, "Should complete all streams"
        assert all(len(r) > 0 for r in responses), "All should have responses"
        assert elapsed < 90, f"Concurrent should be faster, took {elapsed:.2f}s"

        print(f"\nâœ… 3 concurrent streams in {elapsed:.2f}s")
        for i, resp in enumerate(responses):
            print(f"   Stream {i+1}: {resp[:40]}...")

    @pytest.mark.asyncio
    async def test_llm_timeout_handling(self):
        """Test: LLM respects timeout and doesn't hang UI."""
        llm = LLMClient()  # Use default config

        start_time = time.time()

        try:
            # Very long context that might timeout
            response = ""
            async for chunk in llm.stream_chat(
                prompt="Explain quantum physics" * 10, max_tokens=10
            ):
                response += chunk

            elapsed = time.time() - start_time

            # Should either complete fast or timeout gracefully
            assert elapsed < 10, f"Should timeout or complete fast, took {elapsed:.2f}s"

            print(f"\nâœ… Timeout handling working: {elapsed:.2f}s")

        except asyncio.TimeoutError:
            elapsed = time.time() - start_time
            assert elapsed < 10, "Timeout should be enforced"
            print(f"\nâœ… Timeout enforced correctly at {elapsed:.2f}s")

    @pytest.mark.asyncio
    async def test_llm_with_wisdom_system(self):
        """Test: LLM + Wisdom system integration."""
        wisdom = WisdomSystem()
        llm = LLMClient()

        # Get wisdom verse
        verse = wisdom.get_random()

        # Ask LLM about the verse
        prompt = f"In one sentence, what does this mean: {verse.text[:100]}"

        start_time = time.time()
        response = ""
        async for chunk in llm.stream_chat(prompt=prompt, max_tokens=50):
            response += chunk
        elapsed = time.time() - start_time

        assert len(response) > 0, "Should interpret wisdom"
        assert elapsed < 30, f"Should be fast, took {elapsed:.2f}s"

        print("\nâœ… Wisdom + LLM integration:")
        print(f"   Verse: {verse.text[:60]}...")
        print(f"   LLM interpretation: {response[:80]}...")
        print(f"   Time: {elapsed:.2f}s")

    @pytest.mark.asyncio
    @pytest.mark.asyncio
    async def test_ollama_primary_fast(self):
        """Test: Ollama (local) is fastest when available."""
        llm = LLMClient()

        # Force ollama
        start = time.time()
        response = ""
        async for chunk in llm.stream_chat(
            prompt="Quick", provider="ollama", enable_failover=True, max_tokens=5
        ):
            response += chunk
        elapsed = time.time() - start

        # Local should be very fast OR failover to nebius
        assert elapsed < 30, f"Local/failover should be fast: {elapsed:.2f}s"

        print("\nâœ… Primary provider test:")
        print(f"   Time: {elapsed:.2f}s")
        print(f"   Response: {response[:40]}...")

    @pytest.mark.asyncio
    async def test_auto_provider_selection(self):
        """Test: Auto provider selection works."""
        llm = LLMClient()

        start = time.time()
        response = ""
        async for chunk in llm.stream_chat(
            prompt="Auto select",
            provider="auto",  # Let it choose
            max_tokens=10,
        ):
            response += chunk
        elapsed = time.time() - start

        assert len(response) > 0
        assert elapsed < 60

        stats = llm.get_metrics()
        print("\nâœ… Auto provider selection:")
        print(f"   Time: {elapsed:.2f}s")
        print(f"   Providers: {stats.get('providers', {})}")


if __name__ == "__main__":
    print(
        """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     ðŸ§ª TUI + LLM EDGE CASES - SCIENTIFIC TESTING          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Testing with REAL LLM providers:
  1. Ollama (LOCAL) - Primary, fastest
  2. Nebius (ONLINE) - Backup
  3. HuggingFace (ONLINE) - Fallback

Running 8 edge case tests...
    """
    )

    pytest.main([__file__, "-v", "-s"])
