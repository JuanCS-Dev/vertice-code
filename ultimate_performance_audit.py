#!/usr/bin/env python3
"""
üõ°Ô∏è ULTIMATE PERFORMANCE AUDIT - V√âRTICE-CODE 2026
Relat√≥rio Unificado de Performance e Dataflows.
"""

import asyncio
import time
import os
import sys
import psutil
import json
from pathlib import Path
from typing import Dict, List, Any
import logging

# Configura√ß√£o de logging m√≠nima para n√£o poluir o benchmark
logging.basicConfig(level=logging.ERROR)

class UnifiedPerformanceAudit:
    def __init__(self):
        self.results = {
            "timestamp": time.time(),
            "environment": {
                "os": sys.platform,
                "python": sys.version.split()[0],
                "cpu_count": psutil.cpu_count(),
                "total_memory": f"{psutil.virtual_memory().total / (1024**3):.1f} GB"
            },
            "benchmarks": {}
        }

    def log_section(self, title):
        print(f"\n{'='*80}")
        print(f"üöÄ {title}")
        print(f"{ '='*80}")

    async def run_audit(self):
        self.log_section("V√âRTICE-CODE SYSTEM PERFORMANCE AUDIT (2026)")
        
        # 1. Boot & Import Latency
        await self.benchmark_boot_latency()
        
        # 2. Router & Routing Dataflow
        await self.benchmark_routing_logic()
        
        # 3. Agent Handoff & Orchestration
        await self.benchmark_orchestration_overhead()
        
        # 4. Tool System Throughput
        await self.benchmark_tool_system()
        
        # 5. Memory & Session Dataflow
        await self.benchmark_session_management()

        # 6. Final Report
        self.generate_final_report()

    async def benchmark_boot_latency(self):
        print("üì¶ Benchmarking Boot & Import Latency...")
        start = time.perf_counter()
        
        # Simula importa√ß√£o pesada
        from providers import get_router
        router = get_router()
        
        end = time.perf_counter()
        latency = (end - start) * 1000
        
        self.results["benchmarks"]["boot"] = {
            "initialization_ms": latency,
            "status": "üöÄ EXCELLENT" if latency < 500 else "‚úÖ GOOD" if latency < 1500 else "‚ö†Ô∏è SLOW"
        }
        print(f"   ‚Ä¢ Core Router Init: {latency:.2f}ms")

    async def benchmark_routing_logic(self):
        print("üß≠ Benchmarking Routing Dataflow (Router Overhead)...")
        from providers import get_router
        from providers.vertice_router import TaskComplexity, SpeedRequirement
        
        router = get_router()
        iterations = 100
        
        start = time.perf_counter()
        for i in range(iterations):
            router.route(
                task_description=f"Task {i}: Need a complex research on RAG architectures",
                complexity=TaskComplexity.COMPLEX,
                speed=SpeedRequirement.FAST
            )
        end = time.perf_counter()
        
        avg_routing = ((end - start) / iterations) * 1000
        
        self.results["benchmarks"]["routing"] = {
            "avg_routing_overhead_ms": avg_routing,
            "throughput_rps": iterations / (end - start),
            "status": "üöÄ O(1) PERFORMANCE" if avg_routing < 1.0 else "‚úÖ EFFICIENT"
        }
        print(f"   ‚Ä¢ Avg Routing Decision: {avg_routing:.4f}ms")

    async def benchmark_orchestration_overhead(self):
        print("ü§ñ Benchmarking Orchestration & Handoff (Agent Dataflow)...")
        from agents.orchestrator.agent import OrchestratorAgent
        from agents.orchestrator.types import Task, AgentRole
        from providers.vertice_router import TaskComplexity
        
        orchestrator = OrchestratorAgent()
        
        task = Task(
            id="test-task",
            description="Fix a simple bug in auth.py",
            complexity=TaskComplexity.SIMPLE
        )
        
        # Measure handoff creation overhead
        start = time.perf_counter()
        for i in range(100):
            await orchestrator.handoff(task, AgentRole.CODER, "Context data")
        end = time.perf_counter()
        
        handoff_overhead = ((end - start) / 100) * 1000
        
        self.results["benchmarks"]["orchestration"] = {
            "handoff_latency_ms": handoff_overhead,
            "status": "üöÄ ULTRA-LOW" if handoff_overhead < 0.1 else "‚úÖ STABLE"
        }
        print(f"   ‚Ä¢ Agent Handoff Latency: {handoff_overhead:.4f}ms")

    async def benchmark_tool_system(self):
        print("üõ†Ô∏è Benchmarking Tool System Throughput (File IO Dataflow)...")
        from clean_tool_system_v2 import create_clean_mcp_client
        mcp = create_clean_mcp_client()
        
        test_file = "perf_test.tmp"
        content = "x" * 1024 # 1KB
        
        # Write Throughput
        start = time.perf_counter()
        for i in range(50):
            await mcp.call_tool("write_file", {"path": f"{test_file}_{i}", "content": content})
        end = time.perf_counter()
        write_rps = 50 / (end - start)
        
        # Read Throughput
        start = time.perf_counter()
        for i in range(50):
            await mcp.call_tool("read_file", {"path": f"{test_file}_{i}"})
        end = time.perf_counter()
        read_rps = 50 / (end - start)
        
        # Cleanup
        for i in range(50):
            try: os.unlink(f"{test_file}_{i}")
            except: pass
            
        self.results["benchmarks"]["tools"] = {
            "write_ops_sec": write_rps,
            "read_ops_sec": read_rps,
            "status": "üöÄ HIGH-SPEED" if write_rps > 100 else "‚úÖ PRODUCTIVE"
        }
        print(f"   ‚Ä¢ Tool Write Throughput: {write_rps:.1f} ops/sec")
        print(f"   ‚Ä¢ Tool Read Throughput: {read_rps:.1f} ops/sec")

    async def benchmark_session_management(self):
        print("üíæ Benchmarking Session & Memory Dataflow...")
        from vertice_cli.core.session_manager import SessionManager
        
        manager = SessionManager(session_dir="/tmp/vertice_perf_sessions")
        session = manager.start_session()
        
        # Add 100 messages to simulate a long session
        for i in range(100):
            manager.add_message("user", f"Message {i} " + "data "*10)
            manager.add_message("assistant", f"Response {i} " + "more data "*10)
            
        start = time.perf_counter()
        manager.save()
        end = time.perf_counter()
        
        save_latency = (end - start) * 1000
        
        # Measure Memory
        process = psutil.Process()
        memory_usage = process.memory_info().rss / (1024 * 1024) # MB
        
        self.results["benchmarks"]["session"] = {
            "save_latency_ms": save_latency,
            "memory_usage_mb": memory_usage,
            "status": "üöÄ LEAN" if memory_usage < 200 else "‚úÖ NORMAL"
        }
        print(f"   ‚Ä¢ Session Save (200 msgs): {save_latency:.2f}ms")
        print(f"   ‚Ä¢ Process Memory Footprint: {memory_usage:.1f} MB")
        
        # Cleanup
        import shutil
        try: shutil.rmtree("/tmp/vertice_perf_sessions")
        except: pass

    def generate_final_report(self):
        self.log_section("PERFORMANCE AUDIT SUMMARY (V√âRTICE-MAXIMUS)")
        
        print("\nüìä KEY METRICS:")
        b = self.results["benchmarks"]
        print(f"   ‚Ä¢ Core Latency:      {b['boot']['initialization_ms']:>8.2f} ms [{b['boot']['status']}]")
        print(f"   ‚Ä¢ Routing Logic:     {b['routing']['avg_routing_overhead_ms']:>8.4f} ms [{b['routing']['status']}]")
        print(f"   ‚Ä¢ Orchestration:     {b['orchestration']['handoff_latency_ms']:>8.4f} ms [{b['orchestration']['status']}]")
        print(f"   ‚Ä¢ Tool Write:        {b['tools']['write_ops_sec']:>8.1f} ops/s [{b['tools']['status']}]")
        print(f"   ‚Ä¢ Memory Usage:      {b['session']['memory_usage_mb']:>8.1f} MB    [{b['session']['status']}]")
        
        print("\nüèÅ FINAL VERDICT:")
        all_excellent = all("üöÄ" in v["status"] for v in b.values())
        if all_excellent:
            print("   üåü SISTEMA SOBERANO: Performance absoluta detectada em todos os dataflows.")
        else:
            print("   ‚úÖ SISTEMA OTIMIZADO: Pronto para produ√ß√£o Enterprise com excelente efici√™ncia.")

        with open("ultimate_performance_report.json", "w") as f:
            json.dump(self.results, f, indent=2)
        print(f"\nüìù Relat√≥rio detalhado salvo em: ultimate_performance_report.json")

if __name__ == "__main__":
    audit = UnifiedPerformanceAudit()
    asyncio.run(audit.run_audit())
