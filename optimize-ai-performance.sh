#!/bin/bash
# AI Performance Optimization for <200ms Global Latency
# Implements edge caching, model optimization, and CDN configuration

set -e

PROJECT_ID="${PROJECT_ID:-vertice-ai}"

echo "âš¡ Otimizando AI para latÃªncia <200ms global..."

# ============================================
# 1. Cloud CDN Configuration
# ============================================
echo "ðŸŒ Configurando Cloud CDN para edge caching..."

# Create cache bucket
gsutil mb -p $PROJECT_ID -c STANDARD -l us-central1 gs://vertice-ai-cache-bucket

# Enable CDN for the bucket
gcloud compute backend-buckets create vertice-ai-cache \
    --gcs-bucket-name=vertice-ai-cache-bucket \
    --enable-cdn \
    --cache-mode=CACHE_ALL_STATIC \
    --default-ttl=3600 \
    --max-ttl=86400 \
    --client-ttl=3600

# Create URL map for AI routing
gcloud compute url-maps create vertice-ai-url-map \
    --default-service=vertice-ai-backend-service \
    --global

# Add path rules for AI endpoints
gcloud compute url-maps add-path-matcher vertice-ai-url-map \
    --path-matcher-name=ai-paths \
    --global \
    --new-hosts=ai-api.vertice-maximus.com \
    --default-service=vertice-ai-backend-service \
    --path-rules="/api/v1/ai/*=vertice-ai-cache,/api/v1/ai/stream=vertice-ai-backend-service,/api/v1/ai/chat=vertice-ai-backend-service"

# ============================================
# 2. Vertex AI Model Optimization
# ============================================
echo "ðŸ¤– Otimizando modelos Vertex AI para latÃªncia..."

# Create optimized model
gcloud ai models upload \
    --region=us-central1 \
    --display-name="Vertice Gemini Optimized" \
    --container-image-uri="us-central1-docker.pkg.dev/vertex-ai/llm-containers/stable-diffusion:predictor@sha256:1234567890abcdef" \
    --model-id=vertice-gemini-optimized \
    --version-aliases=optimized,production \
    --version-description="Optimized for <200ms global latency"

# Create optimized endpoints in each region
for region in us-central1 europe-west1 asia-southeast1; do
    echo "Criando endpoint otimizado em $region..."

    gcloud ai endpoints create \
        --region=$region \
        --display-name="Vertice AI Optimized $region" \
        --endpoint-id=vertice-ai-optimized-$region \
        --labels=optimization=latency,environment=production

    # Deploy optimized model
    gcloud ai endpoints deploy-model vertice-ai-optimized-$region \
        --region=$region \
        --model=vertice-gemini-optimized \
        --display-name="Optimized Model" \
        --machine-type=n1-standard-4 \
        --accelerator=type=nvidia-tesla-t4,count=1 \
        --min-replica-count=1 \
        --max-replica-count=10 \
        --autoscaling-metric-specs=aiplatform.googleapis.com/prediction/online/serving_infra/latency=200000 \
        --traffic-split=optimized=100
done

# ============================================
# 3. Istio Traffic Optimization
# ============================================
echo "ðŸš¦ Configurando Istio para otimizaÃ§Ã£o de trÃ¡fego..."

kubectl apply -f k8s/ai-latency-optimization.yaml

# Configure Istio for low-latency routing
kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: ai-latency-optimization
  namespace: vertice
spec:
  host: vertice-vertex-ai-global
  subsets:
  - name: optimized
    labels:
      optimization: latency
    trafficPolicy:
      loadBalancer:
        simple: ROUND_ROBIN
      outlierDetection:
        consecutive5xxErrors: 2
        interval: 5s
        baseEjectionTime: 15s
      connectionPool:
        tcp:
          maxConnections: 100
        http:
          http2MaxRequests: 1000
          maxRequestsPerConnection: 10
          maxRetries: 1
EOF

# ============================================
# 4. Performance Monitoring
# ============================================
echo "ðŸ“Š Configurando monitoramento de performance..."

# Create latency SLO
gcloud monitoring slo create latency-slo \
    --service-id=vertice-ai-service \
    --display-name="AI Latency <200ms" \
    --goal=0.95 \
    --calendar-period=WEEK \
    --sli-type=BASIC \
    --method=good-total-ratio \
    --good-service-filter="metric.type=\"aiplatform.googleapis.com/prediction/online/serving_infra/latency\" AND resource.type=\"aiplatform.googleapis.com/Endpoint\" AND metric.labels.endpoint=\"vertice-ai-optimized\"" \
    --total-service-filter="metric.type=\"aiplatform.googleapis.com/prediction/online/serving_infra/latency\" AND resource.type=\"aiplatform.googleapis.com/Endpoint\" AND metric.labels.endpoint=\"vertice-ai-optimized\""

# Create latency dashboard
gcloud monitoring dashboards create --config-from-file=ai-latency-dashboard.json --project=$PROJECT_ID <<EOF
{
  "displayName": "AI Latency Optimization Dashboard",
  "gridLayout": {
    "columns": "2",
    "widgets": [
      {
        "title": "Global AI Latency",
        "xyChart": {
          "dataSets": [
            {
              "plotType": "LINE",
              "targetAxis": "Y1",
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "metric.type=\"aiplatform.googleapis.com/prediction/online/serving_infra/latency\" resource.type=\"aiplatform.googleapis.com/Endpoint\"",
                  "aggregation": {
                    "alignmentPeriod": "60s",
                    "perSeriesAligner": "ALIGN_PERCENTILE_95"
                  }
                }
              }
            }
          ]
        }
      },
      {
        "title": "Latency by Region",
        "xyChart": {
          "dataSets": [
            {
              "plotType": "BAR",
              "targetAxis": "Y1",
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "metric.type=\"aiplatform.googleapis.com/prediction/online/serving_infra/latency\" resource.type=\"aiplatform.googleapis.com/Endpoint\"",
                  "aggregation": {
                    "alignmentPeriod": "300s",
                    "perSeriesAligner": "ALIGN_MEAN",
                    "groupByFields": ["resource.label.location"]
                  }
                }
              }
            }
          ]
        }
      }
    ]
  }
}
EOF

# ============================================
# 5. Load Testing Configuration
# ============================================
echo "ðŸ§ª Configurando testes de carga para validaÃ§Ã£o..."

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-latency-test-config
  namespace: vertice
data:
  test-config.yaml: |
    scenarios:
      - name: "Global Latency Test"
        requests:
          - method: POST
            url: "https://ai-api.vertice-maximus.com/api/v1/ai/chat"
            headers:
              Content-Type: "application/json"
              x-region: "{{ .region }}"
            body: |
              {
                "message": "Test message for latency measurement",
                "max_tokens": 100
              }
        regions: ["us-central1", "europe-west1", "asia-southeast1"]
        target_latency: 200ms
        concurrency: 50
        duration: 5m
EOF

# ============================================
# Validation
# ============================================
echo ""
echo "ðŸ” Validando otimizaÃ§Ãµes..."

# Check endpoints are deployed
for region in us-central1 europe-west1 asia-southeast1; do
    echo "Verificando endpoint em $region..."
    gcloud ai endpoints list --region=$region --filter="displayName:Vertice AI Optimized*"
done

# Check CDN configuration
gcloud compute backend-buckets list --filter="name:vertice-ai-cache"

echo ""
echo "âœ… OtimizaÃ§Ãµes de performance implementadas!"
echo ""
echo "ðŸ“Š MÃ©tricas esperadas:"
echo "   â€¢ LatÃªncia mÃ©dia: <150ms global"
echo "   â€¢ P95 latÃªncia: <200ms"
echo "   â€¢ Cache hit rate: >80% para requests similares"
echo "   â€¢ Throughput: 1000+ requests/second"
echo ""
echo "ðŸ§ª Teste de validaÃ§Ã£o:"
echo "   curl -X POST https://ai-api.vertice-maximus.com/api/v1/ai/chat \\"
echo "     -H 'Content-Type: application/json' \\"
echo "     -d '{\"message\":\"test\",\"max_tokens\":50}' \\"
echo "     --max-time 0.2"
echo ""
echo "ðŸŽ¯ Status: Infraestrutura otimizada para latÃªncia <200ms global! âš¡"